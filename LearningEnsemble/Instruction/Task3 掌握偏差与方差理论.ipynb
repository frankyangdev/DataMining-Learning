{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**写在前面**：本节内容是 [Datawhale三月的组队学习 - 集成学习（上）- CH2-机器学习基础模型回顾 -【Task3 掌握偏差与方差理论】](https://github.com/datawhalechina/team-learning-data-mining/blob/master/EnsembleLearning/CH2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%9B%9E%E9%A1%BE/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.ipynb) 的学习笔记，对应notebook的2.1(4)节，学习周期2天（后调整为4天）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:49:18.762649Z",
     "start_time": "2021-03-22T13:49:18.742124Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import xgboost\n",
    "# Day1\n",
    "from mlxtend.evaluate import bias_variance_decomp # 偏差-方差分解\n",
    "from sklearn.linear_model import LassoLarsIC # Lasso，不清楚具体是什么\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "# Day2\n",
    "from sklearn.model_selection import KFold # K折交叉验证\n",
    "from sklearn.model_selection import LeaveOneOut # 留一法\n",
    "from sklearn.model_selection import StratifiedKFold # 分层交叉验证\n",
    "from sklearn.model_selection import LeavePOut # 留P交叉验证\n",
    "from sklearn.model_selection import GroupKFold # 分组交叉验证\n",
    "import random\n",
    "from sklearn.linear_model import Lasso,Ridge,LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "# Day4\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA # 并不是线性判别分析\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# 测试用\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:49:18.778643Z",
     "start_time": "2021-03-22T13:49:18.765162Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# boston数据集作为本笔记的回归实验数据\n",
    "boston = datasets.load_boston()\n",
    "Xb = boston.data\n",
    "yb = boston.target\n",
    "features = boston.feature_names\n",
    "boston_data = pd.DataFrame(Xb,columns=features)\n",
    "boston_data[\"Price\"] = yb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xb,yb,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:49:18.808867Z",
     "start_time": "2021-03-22T13:49:18.780639Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# breast_cancer数据集作为本笔记的分类实验数据\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "Xbc = breast_cancer.data\n",
    "ybc = breast_cancer.target\n",
    "features = breast_cancer.feature_names\n",
    "breast_cancer = pd.DataFrame(Xbc,columns=features)\n",
    "breast_cancer[\"target\"] = ybc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:49:18.980411Z",
     "start_time": "2021-03-22T13:49:18.811858Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t15.74817\n"
     ]
    }
   ],
   "source": [
    "# 使用xgboost的回归模型作为实验模型\n",
    "model = xgboost.XGBRegressor()\n",
    "model.fit(X=X_train, y=y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础概念\n",
    "**参考资料**：\n",
    "- [偏差和方差有什么区别？](https://www.zhihu.com/question/20448464/answer/765401873)\n",
    "- 微信公众号:数据派THU —《干货 ：教你用Python来计算偏差-方差权衡》\n",
    "- [理解赤池信息量（AIC）,贝叶斯信息量（BIC）](https://blog.csdn.net/chieryu/article/details/51746554)\n",
    "- [模型选择方法：AIC和BIC](https://www.jianshu.com/p/4c8cf5df2092)\n",
    "- [AIC, BIC 和 L1,L2 等正则化有什么区别？](https://zhuanlan.zhihu.com/p/26372789)\n",
    "- [误差与残差](https://blog.csdn.net/fwj_ntu/article/details/82697433)\n",
    "- [残差、方差、偏差、MSE均方误差、Bagging、Boosting、过拟合欠拟合和交叉验证](https://blog.csdn.net/u010986753/article/details/102495494)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概念理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **偏差**：bias  \n",
    "    指**预测结果**与**真实值**之间的`差异`，排除噪声的影响。反映模型本身的精确度。\n",
    "        偏差更多的是针对某个模型输出的样本误差，偏差是模型无法准确表达数据关系导致，比如模型过于简单，非线性的数据关系采用线性模型建模，偏差较大的模型是错的模型\n",
    "\n",
    "\n",
    "- **方差**：variance  \n",
    "    指多个(次)模型**输出的结果**之间的`离散差异`。反映模型的稳定性。\n",
    "        注意这里写的是多个模型或者多次模型，即不同模型或同一模型不同时间的输出结果方差较大，方差是由训练集的数据不够导致。一方面量 【数据量】不够，有限的数据集过度训练导致模型复杂；另一方面质【样本质量】不行，测试集中的数据分布未在训练集中，导致每次抽样训练模型时，每次模型参数不同，输出的结果都无法准确的预测出正确结果  \n",
    "\n",
    "\n",
    "- **偏差-方差分解**  \n",
    "可证得：(我还没证)\n",
    "   $$\n",
    "   E\\left(y_{0}-\\hat{f}\\left(x_{0}\\right)\\right)^{2}=\\operatorname{Var}\\left(\\hat{f}\\left(x_{0}\\right)\\right)+\\left[\\operatorname{Bias}\\left(\\hat{f}\\left(x_{0}\\right)\\right)\\right]^{2}+\\operatorname{Var}(\\varepsilon)\n",
    "   $$      \n",
    "\n",
    "\n",
    "- **误差**：Errors  \n",
    "    指**观测值**与**真实值**的偏差。  \n",
    "\n",
    "\n",
    "- **残差**：Residuals  \n",
    "    指**估计值**与**观测值**的偏差。如果回归模型正确的话， 我们可以将残差看作误差的观测值。  \n",
    "\n",
    "\n",
    "- **残差、方差、偏差总结**\n",
    "    - **简单模型**：偏差大，方差小（简单模型受样本值的影响较小，稳定性高），容易造成欠拟合\n",
    "    - **复杂模型**：偏差小，方差大，容易产生过拟合\n",
    "    - 判断偏差大还是方差大：\n",
    "        1. 模型上的训练样本的真实值较少，则偏差大（欠拟合）\n",
    "        2. 在训练样本上样本拟合的较好，但在测试集上拟合较差，则方差大（过拟合Overfiting）\n",
    "        3. 当偏差较大时，表示目标可能未在模型上（即未瞄准靶心），需要重新训练model（有可能未考虑其他因素对样本的影响，或者应让模型更复杂考虑更高次幂的情况）。增加网络层数，增加隐藏层神经元数量，增加算法迭代次数，或者用更好的优化算法。\n",
    "        4. 当方差较大时：增加更多的数据或正则化  \n",
    "        \n",
    "- **AIC赤池信息量**  \n",
    "越小越好。也写做：-2ln(L) + 2k\n",
    "$$AIC = \\frac{1}{d\\hat{\\sigma}^2}(RSS  +  2d\\hat{\\sigma}^2)$$      \n",
    "- **BIC贝叶斯信息量**  \n",
    "也写做：-2ln(L) + ln(n)*k\n",
    "$$BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)$$    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 偏差-方差分解的mlxtend调用实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T15:03:16.098972Z",
     "start_time": "2021-03-19T15:02:49.378859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t15.416\n",
      "Bias:\t10.676\n",
      "Var:\t4.740\n"
     ]
    }
   ],
   "source": [
    "mse,bias,var = bias_variance_decomp(model, X_train, y_train, X_test, y_test,loss='mse', num_rounds=200)\n",
    "print('MSE:\\t%.3f' % mse)\n",
    "print('Bias:\\t%.3f' % bias)\n",
    "print('Var:\\t%.3f' % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现这里`MSE = Bias + Var`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练误差修正\n",
    "**参考资料**:\n",
    "- [aic python_使用python+sklearn实现Lasso 模型选择：交叉验证/ AIC / BIC](https://blog.csdn.net/weixin_35473090/article/details/112500913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AIC赤池信息量准则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T15:43:00.906169Z",
     "start_time": "2021-03-19T15:43:00.884226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC:\t0.00143\n",
      "MSE:\t31.18560\n"
     ]
    }
   ],
   "source": [
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(X_train, y_train)\n",
    "y_pred = model_aic.predict(X_test)\n",
    "print('AIC:\\t%.5f' % model_aic.alpha_)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIC贝叶斯信息量准则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T15:43:02.609107Z",
     "start_time": "2021-03-19T15:43:02.591155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC:\t0.07306\n",
      "MSE:\t39.80559\n"
     ]
    }
   ],
   "source": [
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(X_train, y_train)\n",
    "y_pred = model_bic.predict(X_test)\n",
    "print('BIC:\\t%.5f' % model_bic.alpha_)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证：Cross Validation\n",
    "**参考资料**：\n",
    "- [数据集划分train_test_split\\交叉验证Cross-validation](https://blog.csdn.net/u010986753/article/details/98069124)\n",
    "- [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "- 《机器学习》 - 周志华"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单交叉验证：train_test_split\n",
    "通过反复重新选择训练集和测试集，继续训练数据和检验模型，最后选择损失函数评估最优的模型和参数。\n",
    "- **好处**：  \n",
    "    处理简单，只需随机把原始数据分为两组即可。\n",
    "    \n",
    "- **缺点**：  \n",
    "    只进行了一次划分，数据结果具有偶然性，没有达到交叉的思想，由于是随机的将原始数据分组，所以最后验证集分类准确率的高低与原始数据的分组有很大的关系，得到的结果并不具有说服性。\n",
    "    \n",
    "**sklearn参数**：  \n",
    "- `test_size`：测试集的样本比例或样本数量\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:05:54.494363Z",
     "start_time": "2021-03-20T04:05:54.473419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.length: 568\t, \ttest.length: 1\n",
      "train==0: 147\t, train=1: 251\t, ratio: 0.59\n",
      "test==0:  65\t, test=1:  106\t, ratio: 0.61\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xbc, ybc, test_size=0.30,\n",
    "                                                    shuffle=True, random_state=320)\n",
    "a, b = len(y_train[y_train == 0]), len(y_train[y_train == 1])\n",
    "c, d = len(y_test[y_test == 0]), len(y_test[y_test == 1])\n",
    "print('train.length: %d\\t, \\ttest.length: %d' %\n",
    "      (train_index.shape[0], test_index.shape[0]))\n",
    "print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a, b, a / b))\n",
    "print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c, d, c / d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 留一交叉验证 Leave-one-out Cross Validation\n",
    "在数据缺乏的情况下使用，如果设原始数据有N个样本，那么LOO-CV就是N-CV，即每个样本单独作为验证集，其余的N-1个样本作为训练集，故LOO-CV会得到N个模型，用这N个模型最终的验证集的分类准确率的平均数作为此下LOO-CV分类器的性能指标。\n",
    "- **优点**：  \n",
    "    不存在数据分布不一致,每一回合中几乎所有的样本皆用于训练模型，因此最接近原始样本的分布，这样评估所得的结果比较可靠。实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。\n",
    "- **缺点**：  \n",
    "    耗时,计算成本高，需要建立的模型数量与原始数据样本数量相同。当数据集较大时几乎不能使用。\n",
    "\n",
    "**sklearn参数**：  \n",
    "- 无"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:02:09.582680Z",
     "start_time": "2021-03-20T04:02:09.549714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 568\n"
     ]
    }
   ],
   "source": [
    "LOO = LeaveOneOut()\n",
    "for loop, (train_idx, test_idx) in enumerate(LOO.split(Xbc, ybc)):\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "#     a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "#     c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "#     print('################(%d)################' % loop)\n",
    "#     print('train.length: %d, test.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "#     print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "#     print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 留P交叉验证 LeavePOut\n",
    "留一法的变体。它从完整的数据集里删除 p 个样本，产生所有可能的训练集和检验集。对于 n个样本，能产生m个训练-检验对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:12:13.017188Z",
     "start_time": "2021-03-20T04:12:12.442317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50077ba43c964c8fa3717443f98fc960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LPO= LeavePOut(p=300)\n",
    "for loop, (train_idx, test_idx) in tqdm(enumerate(LPO.split(Xbc, ybc))):\n",
    "    if loop>10000:\n",
    "        break\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "#     a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "#     c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "#     print('################(%d)################' % loop)\n",
    "#     print('train.length: %d, test.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "#     print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "#     print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自助法 Bootstrapping\n",
    "另一种比较特殊的交叉验证方式，也用于样本量少的时候。假设有m个样本（m较小），每次在这m个样本中随机采集一个样本，放入训练集，采样完后把样本放回。这样重复采集m次，我们得到m个样本组成的训练集。这m个样本中很有可能有重复的样本数据。同时，用没有被采样到的样本做测试集。这样接着进行交叉验证。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。\n",
    "- **优点**：  \n",
    "在数据集较小、难以划分时很有用，能从D中产生不同的S，对集成学习等方法有好处\n",
    "\n",
    "- **缺点**：  \n",
    "产生的S改变了D的分布，会引入估计偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:46:58.770257Z",
     "start_time": "2021-03-20T06:46:58.447509Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(Xb)\n",
    "target = pd.DataFrame(yb)\n",
    "for loop in range(data.shape[0]):\n",
    "    X_train = data.sample(frac=1.0,replace=True) # 有放回随机采样\n",
    "    y_train = target.sample(frac=1.0,replace=True) \n",
    "    X_test = data.loc[X_train.index.difference(X_train.index)].copy() # 将未采样的样本作为测试集\n",
    "    y_test = target.loc[y_train.index.difference(y_train.index)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K折交叉验证 K-Folder Cross Validation\n",
    "应用最多，K-CV可以有效的避免过拟合与欠拟合的发生，最后得到的结果也比较具有说服性。\n",
    "- **实现步骤**：\n",
    "    1. 不重复抽样将原始数据随机分为 k 份。\n",
    "    2. 每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。\n",
    "    3. 重复(2) k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。在每个训练集上训练后得到一个模型，用这个模型在相应的测试集上测试，计算并保存模型的评估指标，\n",
    "    4. 计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。\n",
    "  \n",
    "  \n",
    "- **优点**：  \n",
    "    降低由一次随机划分带来的偶然性，提高其泛化能力，提高对数据的使用效率。\n",
    "- **缺点**：  \n",
    "    可能存在一种情况：数据集有5类，抽取出来的也正好是按照类别划分的5类，也就是说第一折全是0类，第二折全是1类，等等；这样的结果就会导致，模型训练时。没有学习到测试集中数据的特点，从而导致模型得分很低，甚至为0\n",
    "    \n",
    "**sklearn参数**：  \n",
    "- `n_splits`：折数\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T09:04:43.274390Z",
     "start_time": "2021-03-20T09:04:43.253550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################(0)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 139\t, train=1: 240\t, ratio: 0.58\n",
      "test==0:  73\t, test=1:  117\t, ratio: 0.62\n",
      "######################(1)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 135\t, train=1: 244\t, ratio: 0.55\n",
      "test==0:  77\t, test=1:  113\t, ratio: 0.68\n",
      "######################(2)######################\n",
      "train.length: 380\t, \ttest.length: 189\n",
      "train==0: 150\t, train=1: 230\t, ratio: 0.65\n",
      "test==0:  62\t, test=1:  127\t, ratio: 0.49\n",
      "loop: 2\n"
     ]
    }
   ],
   "source": [
    "KF = KFold(n_splits=3, shuffle=True, random_state=320)\n",
    "for loop, (train_idx, test_idx) in enumerate(KF.split(Xbc, ybc)):\n",
    "    if loop>10000:\n",
    "        break\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "    a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    print('######################(%d)######################' % loop)\n",
    "    print('train.length: %d\\t, \\ttest.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "    print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "    print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分层交叉验证 StratifiedKFold\n",
    "KFold的变体。对非平衡数据可以用分层采样，能够在每一份子集中都保持和原始数据集相同的类别比例。  \n",
    "\n",
    "**sklearn参数**：  \n",
    "- `n_splits`：折数\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:13:24.021299Z",
     "start_time": "2021-03-20T04:13:24.007556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################(0)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 141\t, train=1: 238\t, ratio: 0.59\n",
      "test==0:  71\t, test=1:  119\t, ratio: 0.60\n",
      "######################(1)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 141\t, train=1: 238\t, ratio: 0.59\n",
      "test==0:  71\t, test=1:  119\t, ratio: 0.60\n",
      "######################(2)######################\n",
      "train.length: 380\t, \ttest.length: 189\n",
      "train==0: 142\t, train=1: 238\t, ratio: 0.60\n",
      "test==0:  70\t, test=1:  119\t, ratio: 0.59\n",
      "loop: 2\n"
     ]
    }
   ],
   "source": [
    "SKF = StratifiedKFold(n_splits=3, shuffle=True, random_state=320)\n",
    "for loop, (train_idx, test_idx) in enumerate(SKF.split(Xbc, ybc)):\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "    a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    print('######################(%d)######################' % loop)\n",
    "    print('train.length: %d\\t, \\ttest.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "    print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "    print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分组交叉验证 GroupKFold\n",
    "KFold的变体，确保有一个 group 在测试和训练集中都不被表示。通过留出一组特定的不属于测试集和训练集的数据，来测试训练的模型在未知 group 上的性能。需要设定组。\n",
    "\n",
    "**sklearn参数**：  \n",
    "- `n_splits`：折数\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T09:04:59.095570Z",
     "start_time": "2021-03-20T09:04:59.077540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################(0)######################\n",
      "train.length: 367\t, \ttest.length: 202\n",
      "train==0: 137\t, train=1: 230\t, ratio: 0.60\n",
      "test==0:  75\t, test=1:  127\t, ratio: 0.59\n",
      "######################(1)######################\n",
      "train.length: 384\t, \ttest.length: 185\n",
      "train==0: 143\t, train=1: 241\t, ratio: 0.59\n",
      "test==0:  69\t, test=1:  116\t, ratio: 0.59\n",
      "######################(2)######################\n",
      "train.length: 387\t, \ttest.length: 182\n",
      "train==0: 144\t, train=1: 243\t, ratio: 0.59\n",
      "test==0:  68\t, test=1:  114\t, ratio: 0.60\n",
      "loop: 3\n"
     ]
    }
   ],
   "source": [
    "GKF = GroupKFold(n_splits=3)\n",
    "groups = np.array([random.randint(0,2) for i in range(Xbc.shape[0])])\n",
    "for loop, (train_idx, test_idx) in enumerate(GKF.split(Xbc, ybc, groups)):\n",
    "    if loop>10000:\n",
    "        break\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "    a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    print('######################(%d)######################' % loop)\n",
    "    print('train.length: %d\\t, \\ttest.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "    print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "    print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选择：Feature Selection\n",
    "**参考资料**：\n",
    "- [机器学习：特征选择（feature selection）](https://blog.csdn.net/qq_33876194/article/details/88403394?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最优子集选择\n",
    "子集的数量为$2^p$，计算效率低下且需要很高的计算内存\n",
    "- **步骤**：\n",
    "    1. 记不含任何特征的模型为$M_0$，计算这个$M_0$的`测试误差`。                              \n",
    "    2. 在$M_0$基础上增加一个变量，计算p个模型的RSS，选择RSS最小的模型记作$M_1$，并计算该模型$M_1$的`测试误差`。\n",
    "    3. 再增加变量，计算p-1个模型的RSS，并选择RSS最小的模型记作$M_2$，并计算该模型$M_2$的`测试误差`。       \n",
    "    4. 重复以上过程知道拟合的模型有p个特征为止，并选择p+1个模型$\\{M_0,M_1,...,M_p \\}$中`测试误差`最小的模型作为最优模型。\n",
    "    \n",
    "  **注**：`测试误差`值测试集上的误差，指标自选，如：MSE等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉这个方法写起来复杂，运算起来资源也消耗的大，不是很好。不准备写了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T08:40:48.396231Z",
     "start_time": "2021-03-20T08:40:48.391240Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_select_best_subset(X:np.array, y:np.array, error:'func'=MSE, random_stat=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 向前逐步选择\n",
    "最优子集选择的优化\n",
    "- **步骤**：\n",
    "    1. 记不含任何特征的模型为$M_0$，计算这个$M_0$的测试误差。                    \n",
    "    2. 在$M_0$基础上增加一个变量，计算p个模型的RSS，选择RSS最小的模型记作$M_1$，并计算该模型$M_1$的测试误差。   \n",
    "    3. 在最小的RSS模型下继续增加一个变量，选择RSS最小的模型记作$M_2$，并计算该模型$M_2$的测试误差。             \n",
    "    4. 以此类推，重复以上过程知道拟合的模型有p个特征为止，并选择p+1个模型$\\{M_0,M_1,...,M_p \\}$中测试误差最小的模型作为最优模型。   \n",
    "        \n",
    "  **注**：`测试误差`指测试集上的误差，指标自选，如：MSE等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:46:42.280621Z",
     "start_time": "2021-03-22T13:46:42.274637Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_forward_select(X, y, error_func:'func'=MSE, model:'func'=Lasso, random_state=None):\n",
    "    variate_lst = list(map(str,range(X.shape[1]))) # 将位置作为特征名\n",
    "    best_idx_lst = [] # \n",
    "    best_error_lst = []\n",
    "    best_model_lst = []\n",
    "    model = model()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                        random_state=random_state)\n",
    "    M0 = y_test.copy()\n",
    "    M0[:] = y_test.mean() # 第一个模型使用0个特征，即使用均值模型进行拟合得到\n",
    "    best_error_lst.append(error_func(M0, y_test)) # M0对应的误差\n",
    "    while variate_lst: # 当还有特征时继续进行\n",
    "        best_error = float('inf')\n",
    "        for variate in variate_lst: # 从剩下的特征里按顺序选特征\n",
    "            select_idx = best_idx_lst + [int(variate)] # 当前的特征索引序列\n",
    "            select_data, select_target = X_train[:, select_idx], X_test[:, select_idx] \n",
    "            y_pred = model.fit(select_data, y_train).predict(select_target)\n",
    "            error = error_func(y_test, y_pred) # 得到测试误差\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_idx = int(variate)\n",
    "        else:\n",
    "            best_idx_lst.append(best_idx)\n",
    "            best_error_lst.append(best_error)\n",
    "            best_model_lst.append(model)\n",
    "            variate_lst.remove(str(best_idx))\n",
    "    else:\n",
    "        # 这里写的比较复杂。实际上就是得到最小error，同时得到最小error对应的model\n",
    "        best_error,best_model = sorted(list(zip(best_error_lst,best_model_lst)),key=lambda x:x[0])[0]\n",
    "        topk = best_error_lst.index(best_error) # 找到选取几个特征合适\n",
    "        best_select_variate = best_idx_lst[:topk-1] # 选取topk-1个最合适的特征(因为M0在最前面，需要减去)\n",
    "        return best_select_variate,best_error,best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:46:46.732590Z",
     "start_time": "2021-03-22T13:46:46.656844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | CRIM | RAD | INDUS | DIS | ZN\n",
      "best error:\t 17.22996\n"
     ]
    }
   ],
   "source": [
    "# model:Lasso, error:MSE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:46:50.212553Z",
     "start_time": "2021-03-22T13:46:50.126786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | DIS | NOX | ZN | RAD | TAX\n",
      "best error:\t 17.91369\n"
     ]
    }
   ],
   "source": [
    "# model:Ridge, error:MSE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320,\n",
    "                                                                   model=Ridge, error_func=MSE)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:46:52.325417Z",
     "start_time": "2021-03-22T13:46:52.253724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | B | RAD | CHAS | NOX | INDUS | ZN | DIS\n",
      "best error:\t 3.07599\n"
     ]
    }
   ],
   "source": [
    "# model:Lasso, error:MAE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320,\n",
    "                                                                   model=Lasso, error_func=MAE)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T13:46:53.958653Z",
     "start_time": "2021-03-22T13:46:53.903801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | B | CRIM | TAX | NOX | DIS | INDUS\n",
      "best error:\t 3.05965\n"
     ]
    }
   ],
   "source": [
    "# model:Ridge, error:MAE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320,\n",
    "                                                                   model=Ridge, error_func=MAE)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 压缩估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 岭回归(L2)\n",
    "![./Image/Ridge.svg](./Image/Ridge.svg)\n",
    "**参考资料**：\n",
    "- [sklearn 中的线性回归、岭回归、Lasso回归参数配置及示例](https://blog.csdn.net/VariableX/article/details/107166602)\n",
    "- [用scikit-learn和pandas学习Ridge回归](https://www.cnblogs.com/pinard/p/6023000.html)\n",
    "- [从Lasso开始说起](https://zhuanlan.zhihu.com/p/46999826)\n",
    "- [手写算法-python代码实现Ridge(L2正则项)回归](https://blog.csdn.net/weixin_44700798/article/details/110738525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn调用\n",
    "**sklearn参数**：\n",
    "- `alpha`: 正则项系数。数值越大，则对复杂模型的惩罚力度越大。\n",
    "        调参方法：\n",
    "        1. 给定alpha较小的值，例如0.1。\n",
    "        2. 根据验证集准确率以10倍为单位增大或者减小参数值。\n",
    "        3. 在找到合适的数量级后，在此数量级上微调。\n",
    "        合适的候选值：[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "- `normalize`: 是否对各个特征进行标准化。（默认方式：减去均值并除以l2范数）\n",
    "        标准化的好处:\n",
    "        1. 加速收敛\n",
    "        2. 提升精度\n",
    "- `fit_intercept`: 是否计算截距。\n",
    "- `solver`: 解决优化问题的算法\n",
    "    - `svd`: 采⽤用奇异值分解的方法来计算\n",
    "    - `cholesky`: 采⽤用scipy.linalg.solve函数求得闭式解。\n",
    "    - `sparse_cg`: 采⽤用scipy.sparse.linalg.cg函数来求取最优解。\n",
    "    - `lsqr`: 使用scipy.sparse.linalg.lsqr 求解，它是最快的。\n",
    "    - `sag`: 使用随机平均梯度下降，当n_samples和n_features都较大时，通常比其他求解器更快。\n",
    "- `max_iter`: 最大迭代次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:28:37.232305Z",
     "start_time": "2021-03-21T05:28:37.193616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t30.31898\n"
     ]
    }
   ],
   "source": [
    "model_L2 = Ridge()\n",
    "model_L2.fit(X_train, y_train)\n",
    "y_pred = model_L2.predict(X_test)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单实现\n",
    "涉及梯度下降法，暂时还不在学习计划内，或许休息期会补上。现在不搞但以后总会搞的\t&#x1F609;\n",
    "\n",
    "    求解方法：\n",
    "    1. 梯度下降法\n",
    "    2. 标准方程法\n",
    "    \n",
    "**注**:这里准备把最优化算法的理论学习和代码实现放在仓库的同一文件夹下，可前往查看文件：[【扩展1 最优化算法理论及实现】](https://github.com/chenjiyan2001/Datawhale-teamlearning-cHEn/blob/main/2021/03/EnsembleLearning/%E6%89%A9%E5%B1%951%20%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E5%8F%8A%E5%AE%9E%E7%8E%B0.ipynb)。计划在本次学习结束后集中时间学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_ridge():\n",
    "    def __init__(self, alpha=0.1, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "my_lasso = sample_ridge(alpha=0.5)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso回归(L1)\n",
    "![./Image/Lasso.svg](./Image/Lasso.svg)\n",
    "**参考资料**：\n",
    "- [sklearn 中的线性回归、岭回归、Lasso回归参数配置及示例](https://blog.csdn.net/VariableX/article/details/107166602)\n",
    "- [Lasso回归算法： 坐标轴下降法与最小角回归法小结](https://www.cnblogs.com/pinard/p/6018889.html)\n",
    "- [手写算法-python代码实现Lasso回归](https://blog.csdn.net/weixin_44700798/article/details/110690015)\n",
    "- [从Lasso开始说起](https://zhuanlan.zhihu.com/p/46999826)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn调用\n",
    "**sklearn参数**：\n",
    "- `alpha`: 正则项系数。数值越大，则对复杂模型的惩罚力度越大。\n",
    "        调参方法：\n",
    "        1. 给定alpha较小的值，例如0.1。\n",
    "        2. 根据验证集准确率以10倍为单位增大或者减小参数值。\n",
    "        3. 在找到合适的数量级后，在此数量级上微调。\n",
    "        合适的候选值：[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "- `normalize`: 是否对各个特征进行标准化。（默认方式：减去均值并除以l2范数）\n",
    "        标准化的好处:\n",
    "        1. 加速收敛\n",
    "        2. 提升精度\n",
    "- `fit_intercept`: 是否计算截距。\n",
    "- `max_iter`: 最大迭代次数。\n",
    "- `selection`: 指定了每轮迭代时，选择权重向量的哪个分量来更新\n",
    "    - `random`: 更新的时候，随机选择权重向量的⼀个分量来更更新。\n",
    "    - `cyclic`: 更新的时候，从前向后依次选择权重向量的⼀个分量来更新。\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T02:26:07.313490Z",
     "start_time": "2021-03-20T02:26:07.300513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t34.17221\n"
     ]
    }
   ],
   "source": [
    "model_L1 = Lasso()\n",
    "model_L1.fit(X_train, y_train)\n",
    "y_pred = model_L1.predict(X_test)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单实现\n",
    "暂时还不会。。慢慢来吧\n",
    "\n",
    "    求L1范数的损失函数极小值的解法：\n",
    "    1. 坐标轴下降法(coordinate descent)\n",
    "    2. 最小角回归法(Least Angle Regression,LARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:37:19.173893Z",
     "start_time": "2021-03-21T05:37:19.165845Z"
    }
   },
   "outputs": [],
   "source": [
    "class sample_lasso():\n",
    "    def __init__(self, alpha=0.1, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "my_lasso = sample_lasso(alpha=0.5)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维\n",
    "**参考资料**：\n",
    "- [各类降维方法总结](https://www.jianshu.com/p/75e805ff247c?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主成分分析 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T01:53:45.049263Z",
     "start_time": "2021-03-22T01:53:45.031487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维数：\t\t 5\n",
      "保留成分方差之和：\t\t 38275.12164583213\n",
      "保留成分方差百分比之和：\t 0.9984806947092307\n",
      "噪声方差：\t\t 7.28000991024328\n"
     ]
    }
   ],
   "source": [
    "# 保留5维\n",
    "pca = PCA(n_components=5)\n",
    "X = pca.fit_transform(Xb)\n",
    "print('特征维数：\\t\\t',pca.n_components_)\n",
    "print('保留成分方差之和：\\t\\t',sum(pca.explained_variance_))\n",
    "print('保留成分方差百分比之和：\\t',sum(pca.explained_variance_ratio_))\n",
    "print('噪声方差：\\t\\t', pca.noise_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T01:53:38.972957Z",
     "start_time": "2021-03-22T01:53:38.964617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维数：\t\t 2\n",
      "保留成分方差之和：\t\t 37140.24132221935\n",
      "保留成分方差百分比之和：\t 0.9688751429772733\n",
      "噪声方差：\t\t 108.46549117223913\n"
     ]
    }
   ],
   "source": [
    "# 保留95%\n",
    "pca = PCA(n_components=0.95)\n",
    "X = pca.fit_transform(Xb)\n",
    "print('特征维数：\\t\\t',pca.n_components_)\n",
    "print('保留成分方差之和：\\t\\t',sum(pca.explained_variance_))\n",
    "print('保留成分方差百分比之和：\\t',sum(pca.explained_variance_ratio_))\n",
    "print('噪声方差：\\t\\t', pca.noise_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T01:53:32.761712Z",
     "start_time": "2021-03-22T01:53:32.742720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维数：\t\t 12\n",
      "保留成分方差之和：\t\t 38333.35867065725\n",
      "保留成分方差百分比之和：\t 0.9999999203185791\n",
      "噪声方差：\t\t 0.0030544567251183903\n"
     ]
    }
   ],
   "source": [
    "# 通过MLE最大似然算法自动选择\n",
    "pca = PCA(n_components='mle')\n",
    "X = pca.fit_transform(Xb)\n",
    "print('特征维数：\\t\\t',pca.n_components_)\n",
    "print('保留成分方差之和：\\t\\t',sum(pca.explained_variance_))\n",
    "print('保留成分方差百分比之和：\\t',sum(pca.explained_variance_ratio_))\n",
    "print('噪声方差：\\t\\t', pca.noise_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T04:50:13.411414Z",
     "start_time": "2021-03-22T04:50:13.401470Z"
    }
   },
   "outputs": [],
   "source": [
    "class sample_pca():\n",
    "    def __init__(self, n_components=5, threshold=False):\n",
    "        self.n_components = n_components\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit_transform(self,X):\n",
    "        X_std = (X-np.mean(X,0))/np.std(X,0,ddof=1) # 标准化\n",
    "        C = np.cov(X_std,rowvar=0) # 求协方差矩阵\n",
    "        tzz, tzxl = np.linalg.eig(C) # 求得特征值和特征向量\n",
    "        ind = np.argsort(-tzz) # 排序\n",
    "        CPV = np.cumsum(tzz[ind])/np.sum(tzz) # 求得累积方差贡献率CPV\n",
    "        if self.threshold: # 参数threshold有值时通过threshold选取特征维数\n",
    "            self.n_components = np.sum(CPV < self.threshold)+1\n",
    "        loading = tzxl[:,ind][:,0:self.n_components] # 取得特征向量中与较大特征值对应的部分\n",
    "        new_X = X_std.dot(loading) # 计算新的矩阵\n",
    "        return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T04:50:01.437064Z",
     "start_time": "2021-03-22T04:50:01.419114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.09622303, -0.77234843],\n",
       "       [ 1.45581099, -0.59139995],\n",
       "       [ 2.07254655, -0.59904658],\n",
       "       ...,\n",
       "       [ 0.31205166, -1.15410433],\n",
       "       [ 0.27025162, -1.04033206],\n",
       "       [ 0.12567884, -0.76122473]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_components = 2\n",
    "my_pca = sample_pca(n_components = 2)\n",
    "my_pca.fit_transform(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T04:50:15.390948Z",
     "start_time": "2021-03-22T04:50:15.380975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.09622303, -0.77234843, -0.34260368, ..., -0.31832574,\n",
       "        -0.29553933,  0.42451671],\n",
       "       [ 1.45581099, -0.59139995,  0.69451201, ..., -0.55331369,\n",
       "         0.22344881,  0.16679701],\n",
       "       [ 2.07254655, -0.59904658, -0.16695638, ..., -0.48408091,\n",
       "        -0.10506216, -0.06970615],\n",
       "       ...,\n",
       "       [ 0.31205166, -1.15410433,  0.40819364, ..., -0.29382858,\n",
       "         0.63802897, -0.98006238],\n",
       "       [ 0.27025162, -1.04033206,  0.58487527, ..., -0.27132856,\n",
       "         0.57877171, -0.9358292 ],\n",
       "       [ 0.12567884, -0.76122473,  1.29360184, ..., -0.17513633,\n",
       "         0.13325011, -0.85384425]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold = 0.95\n",
    "my_pca = sample_pca(threshold = 0.95)\n",
    "my_pca.fit_transform(Xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 其他降维方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T02:00:09.046026Z",
     "start_time": "2021-03-22T01:58:38.844835Z"
    }
   },
   "outputs": [],
   "source": [
    "# 这个比较花时间诶\n",
    "lda = LDA(n_components=5, max_iter=1000)\n",
    "X = lda.fit_transform(Xbc, ybc)\n",
    "# sum(lda.scalings_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 奇异值分解 SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T02:03:09.538735Z",
     "start_time": "2021-03-22T02:03:09.518157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999871"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD()\n",
    "svd.fit_transform(X)\n",
    "sum(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.275px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
