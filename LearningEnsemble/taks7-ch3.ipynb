{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 投票法的思路"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;投票法是集成学习中常用的技巧，可以帮助我们提高模型的泛化能力，减少模型的错误率。举个例子，在航空航天领域，每个零件发出的电信号都对航空器的成功发射起到重要作用。如果我们有一个二进制形式的信号：\n",
        "\n",
        "11101100100111001011011011011\n",
        "\n",
        "在传输过程中第二位发生了翻转\n",
        "\n",
        "10101100100111001011011011011\n",
        "\n",
        "&emsp;&emsp;这导致的结果可能是致命的。一个常用的纠错方法是重复多次发送数据，并以少数服从多数的方法确定正确的传输数据。一般情况下，错误总是发生在局部，因此融合多个数据是降低误差的一个好方法，这就是投票法的基本思路。\n",
        "\n",
        "&emsp;&emsp;对于回归模型来说，投票法最终的预测结果是多个其他回归模型预测结果的平均值。\n",
        "\n",
        "&emsp;&emsp;对于分类模型，硬投票法的预测结果是多个模型预测结果中出现次数最多的类别，软投票对各类预测结果的概率进行求和，最终选取概率之和最大的类标签。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 投票法的原理分析"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;投票法是一种遵循少数服从多数原则的集成学习模型，通过多个模型的集成降低方差，从而提高模型的鲁棒性。在理想情况下，投票法的预测效果应当优于任何一个基模型的预测效果。\n",
        "\n",
        "&emsp;&emsp;投票法在回归模型与分类模型上均可使用：\n",
        "\n",
        "- 回归投票法：预测结果是所有模型预测结果的平均值。\n",
        "- 分类投票法：预测结果是所有模型种出现最多的预测结果。\n",
        "\n",
        "&emsp;&emsp;分类投票法又可以被划分为硬投票与软投票：\n",
        "\n",
        "- 硬投票：预测结果是所有投票结果最多出现的类。\n",
        "- 软投票：预测结果是所有投票结果中概率加和最大的类。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;下面我们使用一个例子说明硬投票：\n",
        "\n",
        "> 对于某个样本：\n",
        ">\n",
        "> 模型 1 的预测结果是 类别 A\n",
        ">\n",
        "> 模型 2 的预测结果是 类别 B\n",
        ">\n",
        "> 模型 3 的预测结果是 类别 B\n",
        "\n",
        "有2/3的模型预测结果是B，因此硬投票法的预测结果是B\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;同样的例子说明软投票：\n",
        "\n",
        "> 对于某个样本：\n",
        ">\n",
        "> 模型 1 的预测结果是 类别 A 的概率为 99%\n",
        ">\n",
        "> 模型 2 的预测结果是 类别 A 的概率为 49%\n",
        ">\n",
        "> 模型 3 的预测结果是 类别 A 的概率为 49%\n",
        "\n",
        "最终对于类别A的预测概率的平均是 (99 + 49 + 49) / 3 = 65.67%，因此软投票法的预测结果是A。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;从这个例子我们可以看出，软投票法与硬投票法可以得出完全不同的结论。相对于硬投票，软投票法考虑到了预测概率这一额外的信息，因此可以得出比硬投票法更加准确的预测结果。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;在投票法中，我们还需要考虑到不同的基模型可能产生的影响。理论上，基模型可以是任何已被训练好的模型。但在实际应用上，想要投票法产生较好的结果，需要满足两个条件：\n",
        "\n",
        "- 基模型之间的效果不能差别过大。当某个基模型相对于其他基模型效果过差时，该模型很可能成为噪声。\n",
        "- 基模型之间应该有较小的同质性。例如在基模型预测效果近似的情况下，基于树模型与线性模型的投票，往往优于两个树模型或两个线性模型。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "&emsp;&emsp;当投票合集中使用的模型能预测出清晰的类别标签时，适合使用硬投票。当投票集合中使用的模型能预测类别的概率时，适合使用软投票。软投票同样可以用于那些本身并不预测类成员概率的模型，只要他们可以输出类似于概率的预测分数值（例如支持向量机、k-最近邻和决策树）。\n",
        "\n",
        "&emsp;&emsp;投票法的局限性在于，它对所有模型的处理是一样的，这意味着所有模型对预测的贡献是一样的。如果一些模型在某些情况下很好，而在其他情况下很差，这是使用投票法时需要考虑到的一个问题。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 投票法的案例分析(基于sklearn，介绍pipe管道的使用以及voting的使用)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;Sklearn中提供了 [VotingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) 与 [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) 两个投票方法。 &emsp;&emsp;这两种模型的操作方式相同，并采用相同的参数。使用模型需要提供一个模型列表，列表中每个模型采用Tuple的结构表示，第一个元素代表名称，第二个元素代表模型，需要保证每个模型必须拥有唯一的名称。\n",
        "\n",
        "&emsp;&emsp;例如这里，我们定义两个模型："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1618323896524
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [('lr',LogisticRegression()),('svm',SVC())]\n",
        "ensemble = VotingClassifier(estimators=models)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1618323796617
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;有时某些模型需要一些预处理操作，我们可以为他们定义Pipeline完成模型预处理工作："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "models = [('lr',LogisticRegression()),('svm',make_pipeline(StandardScaler(),SVC()))]\n",
        "ensemble = VotingClassifier(estimators=models)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1618323801657
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;模型还提供了voting参数让我们选择软投票或者硬投票："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "models = [('lr',LogisticRegression()),('svm',SVC())]\n",
        "ensemble = VotingClassifier(estimators=models, voting='soft')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1618323805800
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;下面我们使用一个完整的例子演示投票法的使用：\n",
        "\n",
        "&emsp;&emsp;首先我们创建一个1000个样本，20个特征的随机数据集："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "def get_dataset():\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
        "    # summarize the dataset\n",
        "    return X,y"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1618323812431
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;我们使用多个KNN模型作为基模型演示投票法，其中每个模型采用不同的邻居值K参数："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get a voting ensemble of models\n",
        "def get_voting():\n",
        "    # define the base models\n",
        "    models = list()\n",
        "    models.append(('knn1', KNeighborsClassifier(n_neighbors=1)))\n",
        "    models.append(('knn3', KNeighborsClassifier(n_neighbors=3)))\n",
        "    models.append(('knn5', KNeighborsClassifier(n_neighbors=5)))\n",
        "    models.append(('knn7', KNeighborsClassifier(n_neighbors=7)))\n",
        "    models.append(('knn9', KNeighborsClassifier(n_neighbors=9)))\n",
        "    # define the voting ensemble\n",
        "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
        "    return ensemble"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1618323816743
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;然后，我们可以创建一个模型列表来评估投票带来的提升，包括KNN模型配置的每个独立版本和硬投票模型。下面的get_models()函数可以为我们创建模型列表进行评估。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "    models = dict()\n",
        "    models['knn1'] = KNeighborsClassifier(n_neighbors=1)\n",
        "    models['knn3'] = KNeighborsClassifier(n_neighbors=3)\n",
        "    models['knn5'] = KNeighborsClassifier(n_neighbors=5)\n",
        "    models['knn7'] = KNeighborsClassifier(n_neighbors=7)\n",
        "    models['knn9'] = KNeighborsClassifier(n_neighbors=9)\n",
        "    models['hard_voting'] = get_voting()\n",
        "    return models"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1618323822702
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;下面的evaluate_model()函数接收一个模型实例，并以分层10倍交叉验证三次重复的分数列表的形式返回。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate a give model using cross-validation\n",
        "from sklearn.model_selection import cross_val_score   #Added by ljq\n",
        "def evaluate_model(model, X, y):\n",
        "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "    return scores"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1618323900555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;然后，我们可以报告每个算法的平均性能，还可以创建一个箱形图和须状图来比较每个算法的精度分数分布。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from matplotlib import pyplot\n",
        "import statistics\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1618324285962
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset\n",
        "X, y = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "    scores = evaluate_model(model, X, y)\n",
        "    results.append(scores)\n",
        "    names.append(name)\n",
        "    print('>%s %.3f (%.3f)' % (name, statistics.mean(scores), np.std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">knn1 0.873 (0.030)\n",
            ">knn3 0.889 (0.038)\n",
            ">knn5 0.895 (0.031)\n",
            ">knn7 0.899 (0.035)\n",
            ">knn9 0.900 (0.033)\n",
            ">hard_voting 0.902 (0.034)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1618324294690
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;我们得到的结果如下：    \n",
        ">knn1 0.873 (0.030)    \n",
        ">knn3 0.889 (0.038)    \n",
        ">knn5 0.895 (0.031)    \n",
        ">knn7 0.899 (0.035)    \n",
        ">knn9 0.900 (0.033)    \n",
        ">hard_voting 0.902 (0.034)    "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;显然投票的效果略大于任何一个基模型。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;通过箱形图我们可以看到硬投票方法对交叉验证整体预测结果分布带来的提升。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bagging的思路"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;与投票法不同的是，Bagging不仅仅集成模型最后的预测结果，同时采用一定策略来影响基模型训练，保证基模型可以服从一定的假设。在上一章中我们提到，希望各个模型之间具有较大的差异性，而在实际操作中的模型却往往是同质的，因此一个简单的思路是通过不同的采样增加模型的差异性。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bagging的原理分析"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;Bagging的核心在于自助采样(bootstrap)这一概念，即有放回的从数据集中进行采样，也就是说，同样的一个样本可能被多次进行采样。一个自助采样的小例子是我们希望估计全国所有人口年龄的平均值，那么我们可以在全国所有人口中随机抽取不同的集合（这些集合可能存在交集），计算每个集合的平均值，然后将所有平均值的均值作为估计值。\n",
        "\n",
        "&emsp;&emsp;首先我们随机取出一个样本放入采样集合中，再把这个样本放回初始数据集，重复K次采样，最终我们可以获得一个大小为K的样本集合。同样的方法， 我们可以采样出T个含K个样本的采样集合，然后基于每个采样集合训练出一个基学习器，再将这些基学习器进行结合，这就是Bagging的基本流程。\n",
        "\n",
        "&emsp;&emsp;对回归问题的预测是通过预测取平均值来进行的。对于分类问题的预测是通过对预测取多数票预测来进行的。Bagging方法之所以有效，是因为每个模型都是在略微不同的训练数据集上拟合完成的，这又使得每个基模型之间存在略微的差异，使每个基模型拥有略微不同的训练能力。\n",
        "\n",
        "&emsp;&emsp;Bagging同样是一种降低方差的技术，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更加明显。在实际的使用中，加入列采样的Bagging技术对高维小样本往往有神奇的效果。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bagging的案例分析(基于sklearn，介绍随机森林的相关理论以及实例)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;Sklearn为我们提供了 [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) 与 [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) 两种Bagging方法的API，我们在这里通过一个完整的例子演示Bagging在分类问题上的具体应用。这里两种方法的默认基模型是树模型。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;这里的树模型一般指决策树，它是一种树形结构，树的每个非叶子节点表示对样本在一个特征上的判断，节点下方的分支代表对样本的划分。决策树的建立过程是一个对数据不断划分的过程，每次划分中，首先要选择用于划分的特征，之后要确定划分的方案（类别/阈值）。我们希望通过划分，决策树的分支节点所包含的样本“纯度”尽可能地高。节点划分过程中所用的指标主要是信息增益和GINI系数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;信息增益衡量的是划分前后信息不确定性程度的减小。信息不确定程度一般使用信息熵来度量，其计算方式是：\n",
        "$$\n",
        "H(Y) = -\\sum{p_ilogp_i}\n",
        "$$\n",
        "&emsp;&emsp;其中i表示样本的标签，p表示该类样本出现的概率。当我们对样本做出划分之后，计算样本的条件熵：\n",
        "$$\n",
        "H(Y|X) = -\\sum_{x\\in X}p(X=x){H(Y|X=x)}\n",
        "$$\n",
        "\n",
        "&emsp;&emsp;其中x表示用于划分的特征的取值。信息增益定义为信息熵与条件熵的差值：\n",
        "$$\n",
        "IG = H(Y) - H(Y|X)\n",
        "$$\n",
        "\n",
        "&emsp;&emsp;信息增益IG越大，说明使用该特征划分数据所获得的信息量变化越大，子节点的样本“纯度”越高。\n",
        "\n",
        "&emsp;&emsp;同样的，我们也可以利用Gini指数来衡量数据的不纯度，计算方法如下：\n",
        "$$\n",
        "Gini = 1 - \\sum{p_i^2}\n",
        "$$\n",
        "&emsp;&emsp;当我们对样本做出划分后，计算划分后的Gini指数：\n",
        "$$\n",
        "Gini_x = \\sum_{x\\in X}p(X=x)[1 - \\sum{p_i^2}]\n",
        "$$\n",
        "&emsp;&emsp;一般来说，我们选择使得划分后Gini指数最小的特征（注意这里是直接根据Gini指数进行判断，而并非其变化量）。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;下方给出了决策树的一个例子，我们要训练一个模型，根据天气、温度和风力等级来判断是否打网球。    \n",
        "![image-20210411151227186](./DecisionTree.png)\n",
        "\n",
        "首先我们通过计算信息增益或Gini指数确定了首先根据天气情况对样本进行划分，之后对于每个分支，继续考虑除天气之外的其他特征，直到样本的类别被完全分开，所有特征都已使用，或达到树的最大深度为止。\n",
        "\n",
        "Bagging的一个典型应用是随机森林。顾名思义，“森林”是由许多“树”bagging组成的。在具体实现上，用于每个决策树训练的样本和构建决策树的特征都是通过随机采样得到的，随机森林的预测结果是多个决策树输出的组合（投票）。随机森林的示意图如下：\n",
        "\n",
        "![image-20210411161043251](./RandomForest.png)\n",
        "\n",
        "\n",
        "\n",
        "下面我们使用sklearn来实现基于决策树方法的bagging策略。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;我们创建一个含有1000个样本20维特征的随机分类数据集："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
        "                           n_redundant=5, random_state=5)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20) (1000,)\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1618324314582
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;我们将使用重复的分层k-fold交叉验证来评估该模型，一共重复3次，每次有10个fold。我们将评估该模型在所有重复交叉验证中性能的平均值和标准差。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate bagging algorithm for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
        "# define the model\n",
        "model = BaggingClassifier()\n",
        "# evaluate the model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.855 (0.039)\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1618324318626
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;&emsp;最终模型的效果是Accuracy: 0.856 标准差0.037"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}