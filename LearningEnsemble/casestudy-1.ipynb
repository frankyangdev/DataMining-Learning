{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 集成学习案例一 （幸福感预测）\n",
        "\n",
        "### 背景介绍\n",
        "\n",
        "此案例是一个数据挖掘类型的比赛——幸福感预测的baseline。比赛的数据使用的是官方的《中国综合社会调查（CGSS）》文件中的调查结果中的数据，其共包含有139个维度的特征，包括个体变量（性别、年龄、地域、职业、健康、婚姻与政治面貌等等）、家庭变量（父母、配偶、子女、家庭资本等等）、社会态度（公平、信用、公共服务）等特征。\n",
        "\n",
        "\n",
        "### 数据信息\n",
        "赛题要求使用以上 **139** 维的特征，使用 **8000** 余组数据进行对于个人幸福感的预测（预测值为1，2，3，4，5，其中1代表幸福感最低，5代表幸福感最高）。\n",
        "因为考虑到变量个数较多，部分变量间关系复杂，数据分为完整版和精简版两类。可从精简版入手熟悉赛题后，使用完整版挖掘更多信息。在这里我直接使用了完整版的数据。赛题也给出了index文件中包含每个变量对应的问卷题目，以及变量取值的含义；survey文件中为原版问卷，作为补充以方便理解问题背景。\n",
        "\n",
        "### 评价指标\n",
        "最终的评价指标为均方误差MSE，即：\n",
        "$$Score = \\frac{1}{n} \\sum_1 ^n (y_i - y ^*)^2$$\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 导入package"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error,mean_absolute_error, f1_score\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor as rfr\n",
        "from sklearn.ensemble import ExtraTreesRegressor as etr\n",
        "from sklearn.linear_model import BayesianRidge as br\n",
        "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LinearRegression as lr\n",
        "from sklearn.linear_model import ElasticNet as en\n",
        "from sklearn.kernel_ridge import KernelRidge as kr\n",
        "from sklearn.model_selection import  KFold, StratifiedKFold,GroupKFold, RepeatedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore') #消除warning"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1621341361564
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 导入数据集"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"./data/train.csv\", parse_dates=['survey_time'],encoding='latin-1') \n",
        "test = pd.read_csv(\"./data/test.csv\", parse_dates=['survey_time'],encoding='latin-1') #latin-1向下兼容ASCII\n",
        "train = train[train[\"happiness\"]!=-8].reset_index(drop=True)\n",
        "train_data_copy = train.copy() #删去\"happiness\" 为-8的行\n",
        "target_col = \"happiness\" #目标列\n",
        "target = train_data_copy[target_col]\n",
        "del train_data_copy[target_col] #去除目标列\n",
        "\n",
        "data = pd.concat([train_data_copy,test],axis=0,ignore_index=True)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1621341601183
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 查看数据的基本信息"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train.happiness.describe() #数据的基本信息"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "count    7988.000000\nmean        3.867927\nstd         0.818717\nmin         1.000000\n25%         4.000000\n50%         4.000000\n75%         4.000000\nmax         5.000000\nName: happiness, dtype: float64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1621341605755
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据预处理\n",
        "\n",
        "首先需要对于数据中的连续出现的负数值进行处理。由于数据中的负数值只有-1，-2，-3，-8这几种数值，所以它们进行分别的操作，实现代码如下："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#make feature +5\n",
        "#csv中有复数值：-1、-2、-3、-8，将他们视为有问题的特征，但是不删去\n",
        "def getres1(row):\n",
        "    return len([x for x in row.values if type(x)==int and x<0])\n",
        "\n",
        "def getres2(row):\n",
        "    return len([x for x in row.values if type(x)==int and x==-8])\n",
        "\n",
        "def getres3(row):\n",
        "    return len([x for x in row.values if type(x)==int and x==-1])\n",
        "\n",
        "def getres4(row):\n",
        "    return len([x for x in row.values if type(x)==int and x==-2])\n",
        "\n",
        "def getres5(row):\n",
        "    return len([x for x in row.values if type(x)==int and x==-3])\n",
        "\n",
        "#检查数据\n",
        "data['neg1'] = data[data.columns].apply(lambda row:getres1(row),axis=1)\n",
        "data.loc[data['neg1']>20,'neg1'] = 20  #平滑处理\n",
        "\n",
        "data['neg2'] = data[data.columns].apply(lambda row:getres2(row),axis=1)\n",
        "data['neg3'] = data[data.columns].apply(lambda row:getres3(row),axis=1)\n",
        "data['neg4'] = data[data.columns].apply(lambda row:getres4(row),axis=1)\n",
        "data['neg5'] = data[data.columns].apply(lambda row:getres5(row),axis=1)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1621341625539
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "填充缺失值，在这里我采取的方式是将缺失值补全，使用fillna(value)，其中value的数值根据具体的情况来确定。例如将大部分缺失信息认为是零，将家庭成员数认为是1，将家庭收入这个特征认为是66365，即所有家庭的收入平均值。部分实现代码如下："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#填充缺失值 共25列 去掉4列 填充21列\n",
        "#以下的列都是缺省的，视情况填补\n",
        "data['work_status'] = data['work_status'].fillna(0)\n",
        "data['work_yr'] = data['work_yr'].fillna(0)\n",
        "data['work_manage'] = data['work_manage'].fillna(0)\n",
        "data['work_type'] = data['work_type'].fillna(0)\n",
        "\n",
        "data['edu_yr'] = data['edu_yr'].fillna(0)\n",
        "data['edu_status'] = data['edu_status'].fillna(0)\n",
        "\n",
        "data['s_work_type'] = data['s_work_type'].fillna(0)\n",
        "data['s_work_status'] = data['s_work_status'].fillna(0)\n",
        "data['s_political'] = data['s_political'].fillna(0)\n",
        "data['s_hukou'] = data['s_hukou'].fillna(0)\n",
        "data['s_income'] = data['s_income'].fillna(0)\n",
        "data['s_birth'] = data['s_birth'].fillna(0)\n",
        "data['s_edu'] = data['s_edu'].fillna(0)\n",
        "data['s_work_exper'] = data['s_work_exper'].fillna(0)\n",
        "\n",
        "data['minor_child'] = data['minor_child'].fillna(0)\n",
        "data['marital_now'] = data['marital_now'].fillna(0)\n",
        "data['marital_1st'] = data['marital_1st'].fillna(0)\n",
        "data['social_neighbor']=data['social_neighbor'].fillna(0)\n",
        "data['social_friend']=data['social_friend'].fillna(0)\n",
        "data['hukou_loc']=data['hukou_loc'].fillna(1) #最少为1，表示户口\n",
        "data['family_income']=data['family_income'].fillna(66365) #删除问题值后的平均值"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1621341634248
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "除此之外，还有特殊格式的信息需要另外处理，比如与时间有关的信息，这里主要分为两部分进行处理：首先是将“连续”的年龄，进行分层处理，即划分年龄段，具体地在这里我们将年龄分为了6个区间。其次是计算具体的年龄，在Excel表格中，只有出生年月以及调查时间等信息，我们根据此计算出每一位调查者的真实年龄。具体实现代码如下："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#144+1 =145\n",
        "#继续进行特殊的列进行数据处理\n",
        "#读happiness_index.xlsx\n",
        "data['survey_time'] = pd.to_datetime(data['survey_time'], format='%Y-%m-%d',errors='coerce')#防止时间格式不同的报错errors='coerce‘\n",
        "data['survey_time'] = data['survey_time'].dt.year #仅仅是year，方便计算年龄\n",
        "data['age'] = data['survey_time']-data['birth']\n",
        "# print(data['age'],data['survey_time'],data['birth'])\n",
        "#年龄分层 145+1=146\n",
        "bins = [0,17,26,34,50,63,100]\n",
        "data['age_bin'] = pd.cut(data['age'], bins, labels=[0,1,2,3,4,5]) "
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1621341651653
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。第三种方法是使用我们日常生活中的真实情况，例如“宗教信息”特征为负数的认为是“不信仰宗教”，并认为“参加宗教活动的频率”为1，即没有参加过宗教活动，主观的进行补全，这也是我在这一步骤中使用最多的一种方式。就像我自己填表一样，这里我全部都使用了我自己的想法进行缺省值的补全。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#对‘宗教’处理\n",
        "data.loc[data['religion']<0,'religion'] = 1 #1为不信仰宗教\n",
        "data.loc[data['religion_freq']<0,'religion_freq'] = 1 #1为从来没有参加过\n",
        "#对‘教育程度’处理\n",
        "data.loc[data['edu']<0,'edu'] = 4 #初中\n",
        "data.loc[data['edu_status']<0,'edu_status'] = 0\n",
        "data.loc[data['edu_yr']<0,'edu_yr'] = 0\n",
        "#对‘个人收入’处理\n",
        "data.loc[data['income']<0,'income'] = 0 #认为无收入\n",
        "#对‘政治面貌’处理\n",
        "data.loc[data['political']<0,'political'] = 1 #认为是群众\n",
        "#对体重处理\n",
        "data.loc[(data['weight_jin']<=80)&(data['height_cm']>=160),'weight_jin']= data['weight_jin']*2\n",
        "data.loc[data['weight_jin']<=60,'weight_jin']= data['weight_jin']*2  #个人的想法，哈哈哈，没有60斤的成年人吧\n",
        "#对身高处理\n",
        "data.loc[data['height_cm']<150,'height_cm'] = 150 #成年人的实际情况\n",
        "#对‘健康’处理\n",
        "data.loc[data['health']<0,'health'] = 4 #认为是比较健康\n",
        "data.loc[data['health_problem']<0,'health_problem'] = 4\n",
        "#对‘沮丧’处理\n",
        "data.loc[data['depression']<0,'depression'] = 4 #一般人都是很少吧\n",
        "#对‘媒体’处理\n",
        "data.loc[data['media_1']<0,'media_1'] = 1 #都是从不\n",
        "data.loc[data['media_2']<0,'media_2'] = 1\n",
        "data.loc[data['media_3']<0,'media_3'] = 1\n",
        "data.loc[data['media_4']<0,'media_4'] = 1\n",
        "data.loc[data['media_5']<0,'media_5'] = 1\n",
        "data.loc[data['media_6']<0,'media_6'] = 1\n",
        "#对‘空闲活动’处理\n",
        "data.loc[data['leisure_1']<0,'leisure_1'] = 1 #都是根据自己的想法\n",
        "data.loc[data['leisure_2']<0,'leisure_2'] = 5\n",
        "data.loc[data['leisure_3']<0,'leisure_3'] = 3"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1621341660988
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用众数（代码中使用mode()来实现异常值的修正），由于这里的特征是空闲活动，所以采用众数对于缺失值进行处理比较合理。具体的代码参考如下："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[data['leisure_4']<0,'leisure_4'] = data['leisure_4'].mode() #取众数\n",
        "data.loc[data['leisure_5']<0,'leisure_5'] = data['leisure_5'].mode()\n",
        "data.loc[data['leisure_6']<0,'leisure_6'] = data['leisure_6'].mode()\n",
        "data.loc[data['leisure_7']<0,'leisure_7'] = data['leisure_7'].mode()\n",
        "data.loc[data['leisure_8']<0,'leisure_8'] = data['leisure_8'].mode()\n",
        "data.loc[data['leisure_9']<0,'leisure_9'] = data['leisure_9'].mode()\n",
        "data.loc[data['leisure_10']<0,'leisure_10'] = data['leisure_10'].mode()\n",
        "data.loc[data['leisure_11']<0,'leisure_11'] = data['leisure_11'].mode()\n",
        "data.loc[data['leisure_12']<0,'leisure_12'] = data['leisure_12'].mode()\n",
        "data.loc[data['socialize']<0,'socialize'] = 2 #很少\n",
        "data.loc[data['relax']<0,'relax'] = 4 #经常\n",
        "data.loc[data['learn']<0,'learn'] = 1 #从不，哈哈哈哈\n",
        "#对‘社交’处理\n",
        "data.loc[data['social_neighbor']<0,'social_neighbor'] = 0\n",
        "data.loc[data['social_friend']<0,'social_friend'] = 0\n",
        "data.loc[data['socia_outing']<0,'socia_outing'] = 1\n",
        "data.loc[data['neighbor_familiarity']<0,'social_neighbor']= 4\n",
        "#对‘社会公平性’处理\n",
        "data.loc[data['equity']<0,'equity'] = 4\n",
        "#对‘社会等级’处理\n",
        "data.loc[data['class_10_before']<0,'class_10_before'] = 3\n",
        "data.loc[data['class']<0,'class'] = 5\n",
        "data.loc[data['class_10_after']<0,'class_10_after'] = 5\n",
        "data.loc[data['class_14']<0,'class_14'] = 2\n",
        "#对‘工作情况’处理\n",
        "data.loc[data['work_status']<0,'work_status'] = 0\n",
        "data.loc[data['work_yr']<0,'work_yr'] = 0\n",
        "data.loc[data['work_manage']<0,'work_manage'] = 0\n",
        "data.loc[data['work_type']<0,'work_type'] = 0\n",
        "#对‘社会保障’处理\n",
        "data.loc[data['insur_1']<0,'insur_1'] = 1\n",
        "data.loc[data['insur_2']<0,'insur_2'] = 1\n",
        "data.loc[data['insur_3']<0,'insur_3'] = 1\n",
        "data.loc[data['insur_4']<0,'insur_4'] = 1\n",
        "data.loc[data['insur_1']==0,'insur_1'] = 0\n",
        "data.loc[data['insur_2']==0,'insur_2'] = 0\n",
        "data.loc[data['insur_3']==0,'insur_3'] = 0\n",
        "data.loc[data['insur_4']==0,'insur_4'] = 0"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1621341672090
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "取均值进行缺失值的补全（代码实现为means()），在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。具体的代码参考如下："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#对家庭情况处理\n",
        "family_income_mean = data['family_income'].mean()\n",
        "data.loc[data['family_income']<0,'family_income'] = family_income_mean\n",
        "data.loc[data['family_m']<0,'family_m'] = 2\n",
        "data.loc[data['family_status']<0,'family_status'] = 3\n",
        "data.loc[data['house']<0,'house'] = 1\n",
        "data.loc[data['car']<0,'car'] = 0\n",
        "data.loc[data['car']==2,'car'] = 0 #变为0和1\n",
        "data.loc[data['son']<0,'son'] = 1\n",
        "data.loc[data['daughter']<0,'daughter'] = 0\n",
        "data.loc[data['minor_child']<0,'minor_child'] = 0\n",
        "#对‘婚姻’处理\n",
        "data.loc[data['marital_1st']<0,'marital_1st'] = 0\n",
        "data.loc[data['marital_now']<0,'marital_now'] = 0\n",
        "#对‘配偶’处理\n",
        "data.loc[data['s_birth']<0,'s_birth'] = 0\n",
        "data.loc[data['s_edu']<0,'s_edu'] = 0\n",
        "data.loc[data['s_political']<0,'s_political'] = 0\n",
        "data.loc[data['s_hukou']<0,'s_hukou'] = 0\n",
        "data.loc[data['s_income']<0,'s_income'] = 0\n",
        "data.loc[data['s_work_type']<0,'s_work_type'] = 0\n",
        "data.loc[data['s_work_status']<0,'s_work_status'] = 0\n",
        "data.loc[data['s_work_exper']<0,'s_work_exper'] = 0\n",
        "#对‘父母情况’处理\n",
        "data.loc[data['f_birth']<0,'f_birth'] = 1945\n",
        "data.loc[data['f_edu']<0,'f_edu'] = 1\n",
        "data.loc[data['f_political']<0,'f_political'] = 1\n",
        "data.loc[data['f_work_14']<0,'f_work_14'] = 2\n",
        "data.loc[data['m_birth']<0,'m_birth'] = 1940\n",
        "data.loc[data['m_edu']<0,'m_edu'] = 1\n",
        "data.loc[data['m_political']<0,'m_political'] = 1\n",
        "data.loc[data['m_work_14']<0,'m_work_14'] = 2\n",
        "#和同龄人相比社会经济地位\n",
        "data.loc[data['status_peer']<0,'status_peer'] = 2\n",
        "#和3年前比社会经济地位\n",
        "data.loc[data['status_3_before']<0,'status_3_before'] = 2\n",
        "#对‘观点’处理\n",
        "data.loc[data['view']<0,'view'] = 4\n",
        "#对期望年收入处理\n",
        "data.loc[data['inc_ability']<=0,'inc_ability']= 2\n",
        "inc_exp_mean = data['inc_exp'].mean()\n",
        "data.loc[data['inc_exp']<=0,'inc_exp']= inc_exp_mean #取均值\n",
        "\n",
        "#部分特征处理，取众数（首先去除缺失值的数据）\n",
        "for i in range(1,9+1):\n",
        "    data.loc[data['public_service_'+str(i)]<0,'public_service_'+str(i)] = data['public_service_'+str(i)].dropna().mode().values\n",
        "for i in range(1,13+1):\n",
        "    data.loc[data['trust_'+str(i)]<0,'trust_'+str(i)] = data['trust_'+str(i)].dropna().mode().values"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1621341677588
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据增广\n",
        "\n",
        "这一步，我们需要进一步分析每一个特征之间的关系，从而进行数据增广。经过思考，这里我添加了如下的特征：第一次结婚年龄、最近结婚年龄、是否再婚、配偶年龄、配偶年龄差、各种收入比（与配偶之间的收入比、十年后预期收入与现在收入之比等等）、收入与住房面积比（其中也包括10年后期望收入等等各种情况）、社会阶级（10年后的社会阶级、14年后的社会阶级等等）、悠闲指数、满意指数、信任指数等等。除此之外，我还考虑了对于同一省、市、县进行了归一化。例如同一省市内的收入的平均值等以及一个个体相对于同省、市、县其他人的各个指标的情况。同时也考虑了对于同龄人之间的相互比较，即在同龄人中的收入情况、健康情况等等。具体的实现代码如下："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#第一次结婚年龄 147\n",
        "data['marital_1stbir'] = data['marital_1st'] - data['birth'] \n",
        "#最近结婚年龄 148\n",
        "data['marital_nowtbir'] = data['marital_now'] - data['birth'] \n",
        "#是否再婚 149\n",
        "data['mar'] = data['marital_nowtbir'] - data['marital_1stbir']\n",
        "#配偶年龄 150\n",
        "data['marital_sbir'] = data['marital_now']-data['s_birth']\n",
        "#配偶年龄差 151\n",
        "data['age_'] = data['marital_nowtbir'] - data['marital_sbir'] \n",
        "\n",
        "#收入比 151+7 =158\n",
        "data['income/s_income'] = data['income']/(data['s_income']+1) #同居伴侣\n",
        "data['income+s_income'] = data['income']+(data['s_income']+1)\n",
        "data['income/family_income'] = data['income']/(data['family_income']+1)\n",
        "data['all_income/family_income'] = (data['income']+data['s_income'])/(data['family_income']+1)\n",
        "data['income/inc_exp'] = data['income']/(data['inc_exp']+1)\n",
        "data['family_income/m'] = data['family_income']/(data['family_m']+0.01)\n",
        "data['income/m'] = data['income']/(data['family_m']+0.01)\n",
        "\n",
        "#收入/面积比 158+4=162\n",
        "data['income/floor_area'] = data['income']/(data['floor_area']+0.01)\n",
        "data['all_income/floor_area'] = (data['income']+data['s_income'])/(data['floor_area']+0.01)\n",
        "data['family_income/floor_area'] = data['family_income']/(data['floor_area']+0.01)\n",
        "data['floor_area/m'] = data['floor_area']/(data['family_m']+0.01)\n",
        "\n",
        "#class 162+3=165\n",
        "data['class_10_diff'] = (data['class_10_after'] - data['class'])\n",
        "data['class_diff'] = data['class'] - data['class_10_before']\n",
        "data['class_14_diff'] = data['class'] - data['class_14']\n",
        "#悠闲指数 166\n",
        "leisure_fea_lis = ['leisure_'+str(i) for i in range(1,13)]\n",
        "data['leisure_sum'] = data[leisure_fea_lis].sum(axis=1) #skew\n",
        "#满意指数 167\n",
        "public_service_fea_lis = ['public_service_'+str(i) for i in range(1,10)]\n",
        "data['public_service_sum'] = data[public_service_fea_lis].sum(axis=1) #skew\n",
        "\n",
        "#信任指数 168\n",
        "trust_fea_lis = ['trust_'+str(i) for i in range(1,14)]\n",
        "data['trust_sum'] = data[trust_fea_lis].sum(axis=1) #skew\n",
        "\n",
        "#province mean 168+13=181\n",
        "data['province_income_mean'] = data.groupby(['province'])['income'].transform('mean').values\n",
        "data['province_family_income_mean'] = data.groupby(['province'])['family_income'].transform('mean').values\n",
        "data['province_equity_mean'] = data.groupby(['province'])['equity'].transform('mean').values\n",
        "data['province_depression_mean'] = data.groupby(['province'])['depression'].transform('mean').values\n",
        "data['province_floor_area_mean'] = data.groupby(['province'])['floor_area'].transform('mean').values\n",
        "data['province_health_mean'] = data.groupby(['province'])['health'].transform('mean').values\n",
        "data['province_class_10_diff_mean'] = data.groupby(['province'])['class_10_diff'].transform('mean').values\n",
        "data['province_class_mean'] = data.groupby(['province'])['class'].transform('mean').values\n",
        "data['province_health_problem_mean'] = data.groupby(['province'])['health_problem'].transform('mean').values\n",
        "data['province_family_status_mean'] = data.groupby(['province'])['family_status'].transform('mean').values\n",
        "data['province_leisure_sum_mean'] = data.groupby(['province'])['leisure_sum'].transform('mean').values\n",
        "data['province_public_service_sum_mean'] = data.groupby(['province'])['public_service_sum'].transform('mean').values\n",
        "data['province_trust_sum_mean'] = data.groupby(['province'])['trust_sum'].transform('mean').values\n",
        "\n",
        "#city   mean 181+13=194\n",
        "data['city_income_mean'] = data.groupby(['city'])['income'].transform('mean').values #按照city分组\n",
        "data['city_family_income_mean'] = data.groupby(['city'])['family_income'].transform('mean').values\n",
        "data['city_equity_mean'] = data.groupby(['city'])['equity'].transform('mean').values\n",
        "data['city_depression_mean'] = data.groupby(['city'])['depression'].transform('mean').values\n",
        "data['city_floor_area_mean'] = data.groupby(['city'])['floor_area'].transform('mean').values\n",
        "data['city_health_mean'] = data.groupby(['city'])['health'].transform('mean').values\n",
        "data['city_class_10_diff_mean'] = data.groupby(['city'])['class_10_diff'].transform('mean').values\n",
        "data['city_class_mean'] = data.groupby(['city'])['class'].transform('mean').values\n",
        "data['city_health_problem_mean'] = data.groupby(['city'])['health_problem'].transform('mean').values\n",
        "data['city_family_status_mean'] = data.groupby(['city'])['family_status'].transform('mean').values\n",
        "data['city_leisure_sum_mean'] = data.groupby(['city'])['leisure_sum'].transform('mean').values\n",
        "data['city_public_service_sum_mean'] = data.groupby(['city'])['public_service_sum'].transform('mean').values\n",
        "data['city_trust_sum_mean'] = data.groupby(['city'])['trust_sum'].transform('mean').values\n",
        "\n",
        "#county  mean 194 + 13 = 207\n",
        "data['county_income_mean'] = data.groupby(['county'])['income'].transform('mean').values\n",
        "data['county_family_income_mean'] = data.groupby(['county'])['family_income'].transform('mean').values\n",
        "data['county_equity_mean'] = data.groupby(['county'])['equity'].transform('mean').values\n",
        "data['county_depression_mean'] = data.groupby(['county'])['depression'].transform('mean').values\n",
        "data['county_floor_area_mean'] = data.groupby(['county'])['floor_area'].transform('mean').values\n",
        "data['county_health_mean'] = data.groupby(['county'])['health'].transform('mean').values\n",
        "data['county_class_10_diff_mean'] = data.groupby(['county'])['class_10_diff'].transform('mean').values\n",
        "data['county_class_mean'] = data.groupby(['county'])['class'].transform('mean').values\n",
        "data['county_health_problem_mean'] = data.groupby(['county'])['health_problem'].transform('mean').values\n",
        "data['county_family_status_mean'] = data.groupby(['county'])['family_status'].transform('mean').values\n",
        "data['county_leisure_sum_mean'] = data.groupby(['county'])['leisure_sum'].transform('mean').values\n",
        "data['county_public_service_sum_mean'] = data.groupby(['county'])['public_service_sum'].transform('mean').values\n",
        "data['county_trust_sum_mean'] = data.groupby(['county'])['trust_sum'].transform('mean').values\n",
        "\n",
        "#ratio 相比同省 207 + 13 =220\n",
        "data['income/province'] = data['income']/(data['province_income_mean'])                                      \n",
        "data['family_income/province'] = data['family_income']/(data['province_family_income_mean'])   \n",
        "data['equity/province'] = data['equity']/(data['province_equity_mean'])       \n",
        "data['depression/province'] = data['depression']/(data['province_depression_mean'])                                                \n",
        "data['floor_area/province'] = data['floor_area']/(data['province_floor_area_mean'])\n",
        "data['health/province'] = data['health']/(data['province_health_mean'])\n",
        "data['class_10_diff/province'] = data['class_10_diff']/(data['province_class_10_diff_mean'])\n",
        "data['class/province'] = data['class']/(data['province_class_mean'])\n",
        "data['health_problem/province'] = data['health_problem']/(data['province_health_problem_mean'])\n",
        "data['family_status/province'] = data['family_status']/(data['province_family_status_mean'])\n",
        "data['leisure_sum/province'] = data['leisure_sum']/(data['province_leisure_sum_mean'])\n",
        "data['public_service_sum/province'] = data['public_service_sum']/(data['province_public_service_sum_mean'])\n",
        "data['trust_sum/province'] = data['trust_sum']/(data['province_trust_sum_mean']+1)\n",
        "\n",
        "#ratio 相比同市 220 + 13 =233\n",
        "data['income/city'] = data['income']/(data['city_income_mean'])                                      \n",
        "data['family_income/city'] = data['family_income']/(data['city_family_income_mean'])   \n",
        "data['equity/city'] = data['equity']/(data['city_equity_mean'])       \n",
        "data['depression/city'] = data['depression']/(data['city_depression_mean'])                                                \n",
        "data['floor_area/city'] = data['floor_area']/(data['city_floor_area_mean'])\n",
        "data['health/city'] = data['health']/(data['city_health_mean'])\n",
        "data['class_10_diff/city'] = data['class_10_diff']/(data['city_class_10_diff_mean'])\n",
        "data['class/city'] = data['class']/(data['city_class_mean'])\n",
        "data['health_problem/city'] = data['health_problem']/(data['city_health_problem_mean'])\n",
        "data['family_status/city'] = data['family_status']/(data['city_family_status_mean'])\n",
        "data['leisure_sum/city'] = data['leisure_sum']/(data['city_leisure_sum_mean'])\n",
        "data['public_service_sum/city'] = data['public_service_sum']/(data['city_public_service_sum_mean'])\n",
        "data['trust_sum/city'] = data['trust_sum']/(data['city_trust_sum_mean'])\n",
        "\n",
        "#ratio 相比同个地区 233 + 13 =246\n",
        "data['income/county'] = data['income']/(data['county_income_mean'])                                      \n",
        "data['family_income/county'] = data['family_income']/(data['county_family_income_mean'])   \n",
        "data['equity/county'] = data['equity']/(data['county_equity_mean'])       \n",
        "data['depression/county'] = data['depression']/(data['county_depression_mean'])                                                \n",
        "data['floor_area/county'] = data['floor_area']/(data['county_floor_area_mean'])\n",
        "data['health/county'] = data['health']/(data['county_health_mean'])\n",
        "data['class_10_diff/county'] = data['class_10_diff']/(data['county_class_10_diff_mean'])\n",
        "data['class/county'] = data['class']/(data['county_class_mean'])\n",
        "data['health_problem/county'] = data['health_problem']/(data['county_health_problem_mean'])\n",
        "data['family_status/county'] = data['family_status']/(data['county_family_status_mean'])\n",
        "data['leisure_sum/county'] = data['leisure_sum']/(data['county_leisure_sum_mean'])\n",
        "data['public_service_sum/county'] = data['public_service_sum']/(data['county_public_service_sum_mean'])\n",
        "data['trust_sum/county'] = data['trust_sum']/(data['county_trust_sum_mean'])\n",
        "\n",
        "#age   mean 246+ 13 =259\n",
        "data['age_income_mean'] = data.groupby(['age'])['income'].transform('mean').values\n",
        "data['age_family_income_mean'] = data.groupby(['age'])['family_income'].transform('mean').values\n",
        "data['age_equity_mean'] = data.groupby(['age'])['equity'].transform('mean').values\n",
        "data['age_depression_mean'] = data.groupby(['age'])['depression'].transform('mean').values\n",
        "data['age_floor_area_mean'] = data.groupby(['age'])['floor_area'].transform('mean').values\n",
        "data['age_health_mean'] = data.groupby(['age'])['health'].transform('mean').values\n",
        "data['age_class_10_diff_mean'] = data.groupby(['age'])['class_10_diff'].transform('mean').values\n",
        "data['age_class_mean'] = data.groupby(['age'])['class'].transform('mean').values\n",
        "data['age_health_problem_mean'] = data.groupby(['age'])['health_problem'].transform('mean').values\n",
        "data['age_family_status_mean'] = data.groupby(['age'])['family_status'].transform('mean').values\n",
        "data['age_leisure_sum_mean'] = data.groupby(['age'])['leisure_sum'].transform('mean').values\n",
        "data['age_public_service_sum_mean'] = data.groupby(['age'])['public_service_sum'].transform('mean').values\n",
        "data['age_trust_sum_mean'] = data.groupby(['age'])['trust_sum'].transform('mean').values\n",
        "\n",
        "# 和同龄人相比259 + 13 =272\n",
        "data['income/age'] = data['income']/(data['age_income_mean'])                                      \n",
        "data['family_income/age'] = data['family_income']/(data['age_family_income_mean'])   \n",
        "data['equity/age'] = data['equity']/(data['age_equity_mean'])       \n",
        "data['depression/age'] = data['depression']/(data['age_depression_mean'])                                                \n",
        "data['floor_area/age'] = data['floor_area']/(data['age_floor_area_mean'])\n",
        "data['health/age'] = data['health']/(data['age_health_mean'])\n",
        "data['class_10_diff/age'] = data['class_10_diff']/(data['age_class_10_diff_mean'])\n",
        "data['class/age'] = data['class']/(data['age_class_mean'])\n",
        "data['health_problem/age'] = data['health_problem']/(data['age_health_problem_mean'])\n",
        "data['family_status/age'] = data['family_status']/(data['age_family_status_mean'])\n",
        "data['leisure_sum/age'] = data['leisure_sum']/(data['age_leisure_sum_mean'])\n",
        "data['public_service_sum/age'] = data['public_service_sum']/(data['age_public_service_sum_mean'])\n",
        "data['trust_sum/age'] = data['trust_sum']/(data['age_trust_sum_mean'])"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1621341697545
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "经过如上的操作后，最终我们的特征从一开始的131维，扩充为了272维的特征。接下来考虑特征工程、训练模型以及模型融合的工作。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('shape',data.shape)\n",
        "data.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (10956, 272)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "   id  survey_type  province  city  county  survey_time  gender  birth  \\\n0   1            1        12    32      59         2015       1   1959   \n1   2            2        18    52      85         2015       1   1992   \n2   3            2        29    83     126         2015       2   1967   \n3   4            2        10    28      51         2015       2   1943   \n4   5            1         7    18      36         2015       2   1994   \n\n   nationality  religion  ...  depression/age  floor_area/age health/age  \\\n0            1         1  ...        1.285211        0.410351   0.848837   \n1            1         1  ...        0.733333        0.952824   1.179337   \n2            1         0  ...        1.343537        0.972328   1.150485   \n3            1         1  ...        1.111663        0.642329   1.276353   \n4            1         1  ...        0.750000        0.587284   1.177106   \n\n   class_10_diff/age  class/age  health_problem/age  family_status/age  \\\n0           0.000000   0.683307            0.521429           0.733668   \n1           1.012552   1.344444            0.891344           1.359551   \n2           1.190955   1.195762            1.055679           1.190955   \n3           4.977778   1.199143            1.188329           1.162630   \n4           0.000000   0.236957            1.116803           1.093645   \n\n   leisure_sum/age  public_service_sum/age  trust_sum/age  \n0         0.724620                0.666638       0.925941  \n1         1.011792                1.130778       1.188442  \n2         0.966470                1.193204       0.803693  \n3         0.899346                1.153810       1.300950  \n4         1.045313                0.728161       1.117428  \n\n[5 rows x 272 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>survey_type</th>\n      <th>province</th>\n      <th>city</th>\n      <th>county</th>\n      <th>survey_time</th>\n      <th>gender</th>\n      <th>birth</th>\n      <th>nationality</th>\n      <th>religion</th>\n      <th>...</th>\n      <th>depression/age</th>\n      <th>floor_area/age</th>\n      <th>health/age</th>\n      <th>class_10_diff/age</th>\n      <th>class/age</th>\n      <th>health_problem/age</th>\n      <th>family_status/age</th>\n      <th>leisure_sum/age</th>\n      <th>public_service_sum/age</th>\n      <th>trust_sum/age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>12</td>\n      <td>32</td>\n      <td>59</td>\n      <td>2015</td>\n      <td>1</td>\n      <td>1959</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.285211</td>\n      <td>0.410351</td>\n      <td>0.848837</td>\n      <td>0.000000</td>\n      <td>0.683307</td>\n      <td>0.521429</td>\n      <td>0.733668</td>\n      <td>0.724620</td>\n      <td>0.666638</td>\n      <td>0.925941</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>18</td>\n      <td>52</td>\n      <td>85</td>\n      <td>2015</td>\n      <td>1</td>\n      <td>1992</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.733333</td>\n      <td>0.952824</td>\n      <td>1.179337</td>\n      <td>1.012552</td>\n      <td>1.344444</td>\n      <td>0.891344</td>\n      <td>1.359551</td>\n      <td>1.011792</td>\n      <td>1.130778</td>\n      <td>1.188442</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2</td>\n      <td>29</td>\n      <td>83</td>\n      <td>126</td>\n      <td>2015</td>\n      <td>2</td>\n      <td>1967</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1.343537</td>\n      <td>0.972328</td>\n      <td>1.150485</td>\n      <td>1.190955</td>\n      <td>1.195762</td>\n      <td>1.055679</td>\n      <td>1.190955</td>\n      <td>0.966470</td>\n      <td>1.193204</td>\n      <td>0.803693</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2</td>\n      <td>10</td>\n      <td>28</td>\n      <td>51</td>\n      <td>2015</td>\n      <td>2</td>\n      <td>1943</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.111663</td>\n      <td>0.642329</td>\n      <td>1.276353</td>\n      <td>4.977778</td>\n      <td>1.199143</td>\n      <td>1.188329</td>\n      <td>1.162630</td>\n      <td>0.899346</td>\n      <td>1.153810</td>\n      <td>1.300950</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>7</td>\n      <td>18</td>\n      <td>36</td>\n      <td>2015</td>\n      <td>2</td>\n      <td>1994</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.750000</td>\n      <td>0.587284</td>\n      <td>1.177106</td>\n      <td>0.000000</td>\n      <td>0.236957</td>\n      <td>1.116803</td>\n      <td>1.093645</td>\n      <td>1.045313</td>\n      <td>0.728161</td>\n      <td>1.117428</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 272 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1621341706570
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们还应该删去有效样本数很少的特征，例如负值太多的特征或者是缺失值太多的特征，这里我一共删除了包括“目前的最高教育程度”在内的9类特征，得到了最终的263维的特征"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#272-9=263\n",
        "#删除数值特别少的和之前用过的特征\n",
        "del_list=['id','survey_time','edu_other','invest_other','property_other','join_party','province','city','county']\n",
        "use_feature = [clo for clo in data.columns if clo not in del_list]\n",
        "data.fillna(0,inplace=True) #还是补0\n",
        "train_shape = train.shape[0] #一共的数据量，训练集\n",
        "features = data[use_feature].columns #删除后所有的特征\n",
        "X_train_263 = data[:train_shape][use_feature].values\n",
        "y_train = target\n",
        "X_test_263 = data[train_shape:][use_feature].values\n",
        "X_train_263.shape #最终一种263个特征"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "(7988, 263)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1621341715800
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里选择了最重要的49个特征，作为除了以上263维特征外的另外一组特征"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "imp_fea_49 = ['equity','depression','health','class','family_status','health_problem','class_10_after',\n",
        "           'equity/province','equity/city','equity/county',\n",
        "           'depression/province','depression/city','depression/county',\n",
        "           'health/province','health/city','health/county',\n",
        "           'class/province','class/city','class/county',\n",
        "           'family_status/province','family_status/city','family_status/county',\n",
        "           'family_income/province','family_income/city','family_income/county',\n",
        "           'floor_area/province','floor_area/city','floor_area/county',\n",
        "           'leisure_sum/province','leisure_sum/city','leisure_sum/county',\n",
        "           'public_service_sum/province','public_service_sum/city','public_service_sum/county',\n",
        "           'trust_sum/province','trust_sum/city','trust_sum/county',\n",
        "           'income/m','public_service_sum','class_diff','status_3_before','age_income_mean','age_floor_area_mean',\n",
        "           'weight_jin','height_cm',\n",
        "           'health/age','depression/age','equity/age','leisure_sum/age'\n",
        "          ]\n",
        "train_shape = train.shape[0]\n",
        "X_train_49 = data[:train_shape][imp_fea_49].values\n",
        "X_test_49 = data[train_shape:][imp_fea_49].values\n",
        "X_train_49.shape #最重要的49个特征"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "(7988, 49)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1621341722764
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "选择需要进行onehot编码的离散变量进行one-hot编码，再合成为第三类特征，共383维。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cat_fea = ['survey_type','gender','nationality','edu_status','political','hukou','hukou_loc','work_exper','work_status','work_type',\n",
        "           'work_manage','marital','s_political','s_hukou','s_work_exper','s_work_status','s_work_type','f_political','f_work_14',\n",
        "           'm_political','m_work_14'] #已经是0、1的值不需要onehot\n",
        "noc_fea = [clo for clo in use_feature if clo not in cat_fea]\n",
        "\n",
        "onehot_data = data[cat_fea].values\n",
        "enc = preprocessing.OneHotEncoder(categories = 'auto')\n",
        "oh_data=enc.fit_transform(onehot_data).toarray()\n",
        "oh_data.shape #变为onehot编码格式\n",
        "\n",
        "X_train_oh = oh_data[:train_shape,:]\n",
        "X_test_oh = oh_data[train_shape:,:]\n",
        "X_train_oh.shape #其中的训练集\n",
        "\n",
        "X_train_383 = np.column_stack([data[:train_shape][noc_fea].values,X_train_oh])#先是noc，再是cat_fea\n",
        "X_test_383 = np.column_stack([data[train_shape:][noc_fea].values,X_test_oh])\n",
        "X_train_383.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "(7988, 383)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1621341728630
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "基于此，我们构建完成了三种特征工程（训练数据集），其一是上面提取的最重要的49中特征，其中包括健康程度、社会阶级、在同龄人中的收入情况等等特征。其二是扩充后的263维特征（这里可以认为是初始特征）。其三是使用One-hot编码后的特征，这里要使用One-hot进行编码的原因在于，有部分特征为分离值，例如性别中男女，男为1，女为2，我们想使用One-hot将其变为男为0，女为1，来增强机器学习算法的鲁棒性能；再如民族这个特征，原本是1-56这56个数值，如果直接分类会让分类器的鲁棒性变差，所以使用One-hot编码将其变为6个特征进行非零即一的处理。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 特征建模\n",
        "\n",
        "首先我们对于原始的263维的特征，使用lightGBM进行处理，这里我们使用5折交叉验证的方法：\n",
        "\n",
        "1.lightGBM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "##### lgb_263 #\n",
        "#lightGBM决策树\n",
        "lgb_263_param = {\n",
        "'num_leaves': 7, \n",
        "'min_data_in_leaf': 20, #叶子可能具有的最小记录数\n",
        "'objective':'regression',\n",
        "'max_depth': -1,\n",
        "'learning_rate': 0.003,\n",
        "\"boosting\": \"gbdt\", #用gbdt算法\n",
        "\"feature_fraction\": 0.18, #例如 0.18时，意味着在每次迭代中随机选择18％的参数来建树\n",
        "\"bagging_freq\": 1,\n",
        "\"bagging_fraction\": 0.55, #每次迭代时用的数据比例\n",
        "\"bagging_seed\": 14,\n",
        "\"metric\": 'mse',\n",
        "\"lambda_l1\": 0.1,\n",
        "\"lambda_l2\": 0.2, \n",
        "\"verbosity\": -1}\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)   #交叉切分：5\n",
        "oof_lgb_263 = np.zeros(len(X_train_263))\n",
        "predictions_lgb_263 = np.zeros(len(X_test_263))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
        "\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    trn_data = lgb.Dataset(X_train_263[trn_idx], y_train[trn_idx])\n",
        "    val_data = lgb.Dataset(X_train_263[val_idx], y_train[val_idx])#train:val=4:1\n",
        "\n",
        "    num_round = 10000\n",
        "    lgb_263 = lgb.train(lgb_263_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 800)\n",
        "    oof_lgb_263[val_idx] = lgb_263.predict(X_train_263[val_idx], num_iteration=lgb_263.best_iteration)\n",
        "    predictions_lgb_263 += lgb_263.predict(X_test_263, num_iteration=lgb_263.best_iteration) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_263, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "Training until validation scores don't improve for 800 rounds\n",
            "[500]\ttraining's l2: 0.499352\tvalid_1's l2: 0.531888\n",
            "[1000]\ttraining's l2: 0.451093\tvalid_1's l2: 0.499601\n",
            "[1500]\ttraining's l2: 0.42536\tvalid_1's l2: 0.486546\n",
            "[2000]\ttraining's l2: 0.407169\tvalid_1's l2: 0.480212\n",
            "[2500]\ttraining's l2: 0.392787\tvalid_1's l2: 0.477013\n",
            "[3000]\ttraining's l2: 0.380727\tvalid_1's l2: 0.47521\n",
            "[3500]\ttraining's l2: 0.369943\tvalid_1's l2: 0.474274\n",
            "[4000]\ttraining's l2: 0.360216\tvalid_1's l2: 0.473549\n",
            "[4500]\ttraining's l2: 0.351212\tvalid_1's l2: 0.473082\n",
            "[5000]\ttraining's l2: 0.342858\tvalid_1's l2: 0.472917\n",
            "[5500]\ttraining's l2: 0.334923\tvalid_1's l2: 0.47233\n",
            "[6000]\ttraining's l2: 0.327308\tvalid_1's l2: 0.472015\n",
            "[6500]\ttraining's l2: 0.32003\tvalid_1's l2: 0.472013\n",
            "[7000]\ttraining's l2: 0.313146\tvalid_1's l2: 0.471992\n",
            "[7500]\ttraining's l2: 0.306548\tvalid_1's l2: 0.471785\n",
            "[8000]\ttraining's l2: 0.300207\tvalid_1's l2: 0.471628\n",
            "[8500]\ttraining's l2: 0.294075\tvalid_1's l2: 0.471131\n",
            "[9000]\ttraining's l2: 0.28808\tvalid_1's l2: 0.471126\n",
            "Early stopping, best iteration is:\n",
            "[8677]\ttraining's l2: 0.291881\tvalid_1's l2: 0.470808\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 800 rounds\n",
            "[500]\ttraining's l2: 0.504546\tvalid_1's l2: 0.513766\n",
            "[1000]\ttraining's l2: 0.454969\tvalid_1's l2: 0.479654\n",
            "[1500]\ttraining's l2: 0.428829\tvalid_1's l2: 0.466534\n",
            "[2000]\ttraining's l2: 0.411095\tvalid_1's l2: 0.459428\n",
            "[2500]\ttraining's l2: 0.397129\tvalid_1's l2: 0.455549\n",
            "[3000]\ttraining's l2: 0.385324\tvalid_1's l2: 0.452942\n",
            "[3500]\ttraining's l2: 0.374724\tvalid_1's l2: 0.450764\n",
            "[4000]\ttraining's l2: 0.36495\tvalid_1's l2: 0.449467\n",
            "[4500]\ttraining's l2: 0.355734\tvalid_1's l2: 0.448673\n",
            "[5000]\ttraining's l2: 0.347368\tvalid_1's l2: 0.447957\n",
            "[5500]\ttraining's l2: 0.339468\tvalid_1's l2: 0.447045\n",
            "[6000]\ttraining's l2: 0.331802\tvalid_1's l2: 0.446658\n",
            "[6500]\ttraining's l2: 0.324537\tvalid_1's l2: 0.446251\n",
            "[7000]\ttraining's l2: 0.317515\tvalid_1's l2: 0.445686\n",
            "[7500]\ttraining's l2: 0.31089\tvalid_1's l2: 0.445518\n",
            "[8000]\ttraining's l2: 0.304523\tvalid_1's l2: 0.445145\n",
            "[8500]\ttraining's l2: 0.298457\tvalid_1's l2: 0.444951\n",
            "[9000]\ttraining's l2: 0.292543\tvalid_1's l2: 0.444678\n",
            "[9500]\ttraining's l2: 0.286871\tvalid_1's l2: 0.44434\n",
            "[10000]\ttraining's l2: 0.281343\tvalid_1's l2: 0.444136\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[10000]\ttraining's l2: 0.281343\tvalid_1's l2: 0.444136\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 800 rounds\n",
            "[500]\ttraining's l2: 0.503159\tvalid_1's l2: 0.517563\n",
            "[1000]\ttraining's l2: 0.455057\tvalid_1's l2: 0.480185\n",
            "[1500]\ttraining's l2: 0.429509\tvalid_1's l2: 0.464177\n",
            "[2000]\ttraining's l2: 0.411808\tvalid_1's l2: 0.455691\n",
            "[2500]\ttraining's l2: 0.397785\tvalid_1's l2: 0.450164\n",
            "[3000]\ttraining's l2: 0.385727\tvalid_1's l2: 0.446927\n",
            "[3500]\ttraining's l2: 0.375094\tvalid_1's l2: 0.444591\n",
            "[4000]\ttraining's l2: 0.365217\tvalid_1's l2: 0.443095\n",
            "[4500]\ttraining's l2: 0.356076\tvalid_1's l2: 0.442311\n",
            "[5000]\ttraining's l2: 0.347524\tvalid_1's l2: 0.441564\n",
            "[5500]\ttraining's l2: 0.33957\tvalid_1's l2: 0.441105\n",
            "[6000]\ttraining's l2: 0.332015\tvalid_1's l2: 0.441134\n",
            "[6500]\ttraining's l2: 0.324562\tvalid_1's l2: 0.441163\n",
            "Early stopping, best iteration is:\n",
            "[5739]\ttraining's l2: 0.335917\tvalid_1's l2: 0.440989\n",
            "fold n°4\n",
            "Training until validation scores don't improve for 800 rounds\n",
            "[500]\ttraining's l2: 0.504572\tvalid_1's l2: 0.513659\n",
            "[1000]\ttraining's l2: 0.455484\tvalid_1's l2: 0.478731\n",
            "[1500]\ttraining's l2: 0.428738\tvalid_1's l2: 0.465994\n",
            "[2000]\ttraining's l2: 0.410594\tvalid_1's l2: 0.459687\n",
            "[2500]\ttraining's l2: 0.396564\tvalid_1's l2: 0.456166\n",
            "[3000]\ttraining's l2: 0.384555\tvalid_1's l2: 0.453569\n",
            "[3500]\ttraining's l2: 0.373884\tvalid_1's l2: 0.452374\n",
            "[4000]\ttraining's l2: 0.364245\tvalid_1's l2: 0.451174\n",
            "[4500]\ttraining's l2: 0.355307\tvalid_1's l2: 0.450189\n",
            "[5000]\ttraining's l2: 0.346806\tvalid_1's l2: 0.449512\n",
            "[5500]\ttraining's l2: 0.33894\tvalid_1's l2: 0.448986\n",
            "[6000]\ttraining's l2: 0.331377\tvalid_1's l2: 0.448609\n",
            "[6500]\ttraining's l2: 0.324093\tvalid_1's l2: 0.447908\n",
            "[7000]\ttraining's l2: 0.317279\tvalid_1's l2: 0.447825\n",
            "[7500]\ttraining's l2: 0.310538\tvalid_1's l2: 0.447602\n",
            "[8000]\ttraining's l2: 0.304056\tvalid_1's l2: 0.447361\n",
            "[8500]\ttraining's l2: 0.297937\tvalid_1's l2: 0.447314\n",
            "[9000]\ttraining's l2: 0.29201\tvalid_1's l2: 0.447308\n",
            "[9500]\ttraining's l2: 0.286206\tvalid_1's l2: 0.447376\n",
            "Early stopping, best iteration is:\n",
            "[8730]\ttraining's l2: 0.295264\tvalid_1's l2: 0.447178\n",
            "fold n°5\n",
            "Training until validation scores don't improve for 800 rounds\n",
            "[500]\ttraining's l2: 0.50329\tvalid_1's l2: 0.520118\n",
            "[1000]\ttraining's l2: 0.454678\tvalid_1's l2: 0.485048\n",
            "[1500]\ttraining's l2: 0.428836\tvalid_1's l2: 0.471513\n",
            "[2000]\ttraining's l2: 0.410698\tvalid_1's l2: 0.464435\n",
            "[2500]\ttraining's l2: 0.396313\tvalid_1's l2: 0.460812\n",
            "[3000]\ttraining's l2: 0.383887\tvalid_1's l2: 0.458692\n",
            "[3500]\ttraining's l2: 0.372911\tvalid_1's l2: 0.457729\n",
            "[4000]\ttraining's l2: 0.363121\tvalid_1's l2: 0.456941\n",
            "[4500]\ttraining's l2: 0.354021\tvalid_1's l2: 0.456579\n",
            "[5000]\ttraining's l2: 0.345558\tvalid_1's l2: 0.456347\n",
            "[5500]\ttraining's l2: 0.337527\tvalid_1's l2: 0.456801\n",
            "Early stopping, best iteration is:\n",
            "[4992]\ttraining's l2: 0.345691\tvalid_1's l2: 0.456276\n",
            "CV score: 0.45187741\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1621341782714
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "接着，我使用已经训练完的lightGBM的模型进行特征重要性的判断以及可视化，从结果我们可以看出，排在重要性第一位的是health/age，就是同龄人中的健康程度，与我们主观的看法基本一致。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------特征重要性\n",
        "pd.set_option('display.max_columns', None)\n",
        "#显示所有行\n",
        "pd.set_option('display.max_rows', None)\n",
        "#设置value的显示长度为100，默认为50\n",
        "pd.set_option('max_colwidth',100)\n",
        "df = pd.DataFrame(data[use_feature].columns.tolist(), columns=['feature'])\n",
        "df['importance']=list(lgb_263.feature_importance())\n",
        "df = df.sort_values(by='importance',ascending=False)\n",
        "plt.figure(figsize=(14,28))\n",
        "sns.barplot(x=\"importance\", y=\"feature\", data=df.head(50))\n",
        "plt.title('Features importance (averaged/folds)')\n",
        "plt.tight_layout()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1008x2016 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1621341799001
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "后面，我们使用常见的机器学习方法，对于263维特征进行建模：\n",
        "\n",
        "2.xgboost"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "##### xgb_263\n",
        "#xgboost\n",
        "xgb_263_params = {'eta': 0.02,  #lr\n",
        "              'max_depth': 6,  \n",
        "              'min_child_weight':3,#最小叶子节点样本权重和\n",
        "              'gamma':0, #指定节点分裂所需的最小损失函数下降值。\n",
        "              'subsample': 0.7,  #控制对于每棵树，随机采样的比例\n",
        "              'colsample_bytree': 0.3,  #用来控制每棵随机采样的列数的占比 (每一列是一个特征)。\n",
        "              'lambda':2,\n",
        "              'objective': 'reg:linear', \n",
        "              'eval_metric': 'rmse', \n",
        "              'silent': True, \n",
        "              'nthread': -1}\n",
        "\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
        "oof_xgb_263 = np.zeros(len(X_train_263))\n",
        "predictions_xgb_263 = np.zeros(len(X_test_263))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    trn_data = xgb.DMatrix(X_train_263[trn_idx], y_train[trn_idx])\n",
        "    val_data = xgb.DMatrix(X_train_263[val_idx], y_train[val_idx])\n",
        "\n",
        "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
        "    xgb_263 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_263_params)\n",
        "    oof_xgb_263[val_idx] = xgb_263.predict(xgb.DMatrix(X_train_263[val_idx]), ntree_limit=xgb_263.best_ntree_limit)\n",
        "    predictions_xgb_263 += xgb_263.predict(xgb.DMatrix(X_test_263), ntree_limit=xgb_263.best_ntree_limit) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_263, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "[0]\ttrain-rmse:3.39997\tvalid_data-rmse:3.40021\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.408711\tvalid_data-rmse:0.686147\n",
            "[1000]\ttrain-rmse:0.275616\tvalid_data-rmse:0.685248\n",
            "Stopping. Best iteration:\n",
            "[705]\ttrain-rmse:0.348482\tvalid_data-rmse:0.683704\n",
            "\n",
            "fold n°2\n",
            "[0]\ttrain-rmse:3.39998\tvalid_data-rmse:3.40027\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.411171\tvalid_data-rmse:0.676784\n",
            "[1000]\ttrain-rmse:0.275916\tvalid_data-rmse:0.679\n",
            "Stopping. Best iteration:\n",
            "[471]\ttrain-rmse:0.420548\tvalid_data-rmse:0.676549\n",
            "\n",
            "fold n°3\n",
            "[0]\ttrain-rmse:3.40001\tvalid_data-rmse:3.40015\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.409421\tvalid_data-rmse:0.670584\n",
            "[1000]\ttrain-rmse:0.271153\tvalid_data-rmse:0.67233\n",
            "Stopping. Best iteration:\n",
            "[579]\ttrain-rmse:0.383438\tvalid_data-rmse:0.670081\n",
            "\n",
            "fold n°4\n",
            "[0]\ttrain-rmse:3.4\tvalid_data-rmse:3.40037\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.411916\tvalid_data-rmse:0.669253\n",
            "[1000]\ttrain-rmse:0.277582\tvalid_data-rmse:0.66848\n",
            "Stopping. Best iteration:\n",
            "[758]\ttrain-rmse:0.337674\tvalid_data-rmse:0.667901\n",
            "\n",
            "fold n°5\n",
            "[0]\ttrain-rmse:3.40019\tvalid_data-rmse:3.40013\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.413762\tvalid_data-rmse:0.66008\n",
            "[1000]\ttrain-rmse:0.278698\tvalid_data-rmse:0.66154\n",
            "Stopping. Best iteration:\n",
            "[409]\ttrain-rmse:0.443753\tvalid_data-rmse:0.65956\n",
            "\n",
            "CV score: 0.45106048\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1621342007934
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. RandomForestRegressor随机森林"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#RandomForestRegressor随机森林\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
        "oof_rfr_263 = np.zeros(len(X_train_263))\n",
        "predictions_rfr_263 = np.zeros(len(X_test_263))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_263[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    rfr_263 = rfr(n_estimators=1600,max_depth=9, min_samples_leaf=9, min_weight_fraction_leaf=0.0,\n",
        "            max_features=0.25,verbose=1,n_jobs=-1) #并行化\n",
        "    #verbose = 0 为不在标准输出流输出日志信息\n",
        "#verbose = 1 为输出进度条记录\n",
        "#verbose = 2 为每个epoch输出一行记录\n",
        "    rfr_263.fit(tr_x,tr_y)\n",
        "    oof_rfr_263[val_idx] = rfr_263.predict(X_train_263[val_idx])\n",
        "    \n",
        "    predictions_rfr_263 += rfr_263.predict(X_test_263) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_rfr_263, target)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. GradientBoostingRegressor梯度提升决策树"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#GradientBoostingRegressor梯度提升决策树\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
        "oof_gbr_263 = np.zeros(train_shape)\n",
        "predictions_gbr_263 = np.zeros(len(X_test_263))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_263[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    gbr_263 = gbr(n_estimators=400, learning_rate=0.01,subsample=0.65,max_depth=7, min_samples_leaf=20,\n",
        "            max_features=0.22,verbose=1)\n",
        "    gbr_263.fit(tr_x,tr_y)\n",
        "    oof_gbr_263[val_idx] = gbr_263.predict(X_train_263[val_idx])\n",
        "    \n",
        "    predictions_gbr_263 += gbr_263.predict(X_test_263) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_263, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6419           0.0036           24.34s\n",
            "         2           0.6564           0.0031           23.18s\n",
            "         3           0.6693           0.0031           22.69s\n",
            "         4           0.6589           0.0031           22.78s\n",
            "         5           0.6522           0.0027           22.58s\n",
            "         6           0.6521           0.0031           22.40s\n",
            "         7           0.6370           0.0029           22.23s\n",
            "         8           0.6343           0.0030           22.06s\n",
            "         9           0.6447           0.0029           21.87s\n",
            "        10           0.6397           0.0028           21.75s\n",
            "        20           0.5955           0.0019           20.93s\n",
            "        30           0.5695           0.0016           20.09s\n",
            "        40           0.5460           0.0015           19.34s\n",
            "        50           0.5121           0.0011           18.65s\n",
            "        60           0.4994           0.0012           18.03s\n",
            "        70           0.4912           0.0010           17.44s\n",
            "        80           0.4719           0.0010           16.76s\n",
            "        90           0.4310           0.0007           16.28s\n",
            "       100           0.4437           0.0006           15.84s\n",
            "       200           0.3424           0.0002           10.15s\n",
            "       300           0.3063          -0.0000            4.94s\n",
            "       400           0.2759          -0.0000            0.00s\n",
            "fold n°2\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6836           0.0034           24.61s\n",
            "         2           0.6613           0.0030           22.86s\n",
            "         3           0.6500           0.0031           24.11s\n",
            "         4           0.6621           0.0036           23.15s\n",
            "         5           0.6356           0.0031           23.49s\n",
            "         6           0.6460           0.0029           23.13s\n",
            "         7           0.6263           0.0032           22.83s\n",
            "         8           0.6149           0.0029           22.72s\n",
            "         9           0.6350           0.0030           22.83s\n",
            "        10           0.6325           0.0026           22.65s\n",
            "        20           0.6064           0.0025           21.62s\n",
            "        30           0.5812           0.0018           20.59s\n",
            "        40           0.5460           0.0018           19.98s\n",
            "        50           0.5016           0.0014           19.52s\n",
            "        60           0.4991           0.0010           18.84s\n",
            "        70           0.4645           0.0009           18.24s\n",
            "        80           0.4621           0.0007           17.76s\n",
            "        90           0.4497           0.0007           17.20s\n",
            "       100           0.4374           0.0005           16.51s\n",
            "       200           0.3420           0.0001           10.35s\n",
            "       300           0.3032          -0.0000            4.95s\n",
            "       400           0.2710          -0.0000            0.00s\n",
            "fold n°3\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6692           0.0036           24.95s\n",
            "         2           0.6468           0.0031           23.99s\n",
            "         3           0.6313           0.0034           24.05s\n",
            "         4           0.6499           0.0032           23.70s\n",
            "         5           0.6358           0.0033           23.38s\n",
            "         6           0.6343           0.0029           23.05s\n",
            "         7           0.6312           0.0036           22.71s\n",
            "         8           0.6180           0.0032           22.47s\n",
            "         9           0.6275           0.0035           22.57s\n",
            "        10           0.6168           0.0030           22.24s\n",
            "        20           0.5792           0.0021           20.73s\n",
            "        30           0.5583           0.0023           20.27s\n",
            "        40           0.5521           0.0018           19.70s\n",
            "        50           0.5067           0.0013           18.84s\n",
            "        60           0.4754           0.0010           18.42s\n",
            "        70           0.4811           0.0009           17.84s\n",
            "        80           0.4603           0.0008           17.38s\n",
            "        90           0.4439           0.0006           16.74s\n",
            "       100           0.4323           0.0007           16.25s\n",
            "       200           0.3401           0.0002           10.23s\n",
            "       300           0.2862          -0.0000            4.84s\n",
            "       400           0.2690          -0.0000            0.00s\n",
            "fold n°4\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6687           0.0032           21.09s\n",
            "         2           0.6517           0.0031           23.29s\n",
            "         3           0.6583           0.0031           23.63s\n",
            "         4           0.6607           0.0033           24.45s\n",
            "         5           0.6583           0.0029           24.78s\n",
            "         6           0.6688           0.0028           24.80s\n",
            "         7           0.6320           0.0030           25.08s\n",
            "         8           0.6502           0.0026           24.94s\n",
            "         9           0.6358           0.0026           24.51s\n",
            "        10           0.6258           0.0027           24.24s\n",
            "        20           0.5910           0.0023           22.41s\n",
            "        30           0.5609           0.0020           21.31s\n",
            "        40           0.5399           0.0017           20.50s\n",
            "        50           0.4963           0.0013           19.67s\n",
            "        60           0.4844           0.0012           18.86s\n",
            "        70           0.4781           0.0008           18.21s\n",
            "        80           0.4484           0.0010           17.63s\n",
            "        90           0.4619           0.0006           16.95s\n",
            "       100           0.4430           0.0005           16.46s\n",
            "       200           0.3377           0.0001           10.50s\n",
            "       300           0.3001           0.0001            4.97s\n",
            "       400           0.2623          -0.0000            0.00s\n",
            "fold n°5\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6857           0.0031           23.50s\n",
            "         2           0.6320           0.0035           24.26s\n",
            "         3           0.6573           0.0033           23.41s\n",
            "         4           0.6494           0.0033           24.20s\n",
            "         5           0.6311           0.0033           24.32s\n",
            "         6           0.6362           0.0031           24.20s\n",
            "         7           0.6291           0.0032           24.05s\n",
            "         8           0.6354           0.0032           23.56s\n",
            "         9           0.6383           0.0030           23.54s\n",
            "        10           0.6250           0.0029           23.64s\n",
            "        20           0.5989           0.0023           21.45s\n",
            "        30           0.5736           0.0019           20.27s\n",
            "        40           0.5457           0.0016           19.60s\n",
            "        50           0.5045           0.0015           18.76s\n",
            "        60           0.4820           0.0012           18.20s\n",
            "        70           0.4756           0.0010           17.44s\n",
            "        80           0.4484           0.0009           16.91s\n",
            "        90           0.4410           0.0007           16.34s\n",
            "       100           0.4195           0.0004           15.72s\n",
            "       200           0.3348           0.0001           10.05s\n",
            "       300           0.2933          -0.0000            4.76s\n",
            "       400           0.2658          -0.0000            0.00s\n",
            "CV score: 0.45583290\n"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. ExtraTreesRegressor 极端随机森林回归"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#ExtraTreesRegressor 极端随机森林回归\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_etr_263 = np.zeros(train_shape)\n",
        "predictions_etr_263 = np.zeros(len(X_test_263))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_263[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    etr_263 = etr(n_estimators=1000,max_depth=8, min_samples_leaf=12, min_weight_fraction_leaf=0.0,\n",
        "            max_features=0.4,verbose=1,n_jobs=-1)# max_feature：划分时考虑的最大特征数\n",
        "    etr_263.fit(tr_x,tr_y)\n",
        "    oof_etr_263[val_idx] = etr_263.predict(X_train_263[val_idx])\n",
        "    \n",
        "    predictions_etr_263 += etr_263.predict(X_test_263) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_etr_263, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.0s\n",
            "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    9.0s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    3.8s\n",
            "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    6.9s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    8.9s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.1s\n",
            "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    7.6s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    9.6s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.0s\n",
            "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    7.6s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   10.6s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.4s\n",
            "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.6s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   10.7s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV score: 0.48598792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此，我们得到了以上5种模型的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_stack2 = np.vstack([oof_lgb_263,oof_xgb_263,oof_gbr_263,oof_rfr_263,oof_etr_263]).transpose()\n",
        "# transpose()函数的作用就是调换x,y,z的位置,也就是数组的索引值\n",
        "test_stack2 = np.vstack([predictions_lgb_263, predictions_xgb_263,predictions_gbr_263,predictions_rfr_263,predictions_etr_263]).transpose()\n",
        "\n",
        "#交叉验证:5折，重复2次\n",
        "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
        "oof_stack2 = np.zeros(train_stack2.shape[0])\n",
        "predictions_lr2 = np.zeros(test_stack2.shape[0])\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack2,target)):\n",
        "    print(\"fold {}\".format(fold_))\n",
        "    trn_data, trn_y = train_stack2[trn_idx], target.iloc[trn_idx].values\n",
        "    val_data, val_y = train_stack2[val_idx], target.iloc[val_idx].values\n",
        "    #Kernel Ridge Regression\n",
        "    lr2 = kr()\n",
        "    lr2.fit(trn_data, trn_y)\n",
        "    \n",
        "    oof_stack2[val_idx] = lr2.predict(val_data)\n",
        "    predictions_lr2 += lr2.predict(test_stack2) / 10\n",
        "    \n",
        "mean_squared_error(target.values, oof_stack2) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0\n",
            "fold 1\n",
            "fold 2\n",
            "fold 3\n",
            "fold 4\n",
            "fold 5\n",
            "fold 6\n",
            "fold 7\n",
            "fold 8\n",
            "fold 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "0.44815130114230267"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下来我们对于49维的数据进行与上述263维数据相同的操作\n",
        "\n",
        "1.lightGBM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "##### lgb_49\n",
        "lgb_49_param = {\n",
        "'num_leaves': 9,\n",
        "'min_data_in_leaf': 23,\n",
        "'objective':'regression',\n",
        "'max_depth': -1,\n",
        "'learning_rate': 0.002,\n",
        "\"boosting\": \"gbdt\",\n",
        "\"feature_fraction\": 0.45, \n",
        "\"bagging_freq\": 1,\n",
        "\"bagging_fraction\": 0.65, \n",
        "\"bagging_seed\": 15,\n",
        "\"metric\": 'mse',\n",
        "\"lambda_l2\": 0.2, \n",
        "\"verbosity\": -1} # 一个叶子上数据的最小数量 \\ feature_fraction将会在每棵树训练之前选择 45% 的特征。可以用来加速训练，可以用来处理过拟合。 #bagging_fraction不进行重采样的情况下随机选择部分数据。可以用来加速训练，可以用来处理过拟合。\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)   \n",
        "oof_lgb_49 = np.zeros(len(X_train_49))\n",
        "predictions_lgb_49 = np.zeros(len(X_test_49))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    trn_data = lgb.Dataset(X_train_49[trn_idx], y_train[trn_idx])\n",
        "    val_data = lgb.Dataset(X_train_49[val_idx], y_train[val_idx])\n",
        "\n",
        "    num_round = 12000\n",
        "    lgb_49 = lgb.train(lgb_49_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
        "    oof_lgb_49[val_idx] = lgb_49.predict(X_train_49[val_idx], num_iteration=lgb_49.best_iteration)\n",
        "    predictions_lgb_49 += lgb_49.predict(X_test_49, num_iteration=lgb_49.best_iteration) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_49, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[1000]\ttraining's l2: 0.46958\tvalid_1's l2: 0.500767\n",
            "[2000]\ttraining's l2: 0.429395\tvalid_1's l2: 0.482214\n",
            "[3000]\ttraining's l2: 0.406748\tvalid_1's l2: 0.477959\n",
            "[4000]\ttraining's l2: 0.388735\tvalid_1's l2: 0.476283\n",
            "[5000]\ttraining's l2: 0.373399\tvalid_1's l2: 0.475506\n",
            "[6000]\ttraining's l2: 0.359798\tvalid_1's l2: 0.475435\n",
            "Early stopping, best iteration is:\n",
            "[5429]\ttraining's l2: 0.367348\tvalid_1's l2: 0.475325\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[1000]\ttraining's l2: 0.469767\tvalid_1's l2: 0.496741\n",
            "[2000]\ttraining's l2: 0.428546\tvalid_1's l2: 0.479198\n",
            "[3000]\ttraining's l2: 0.405733\tvalid_1's l2: 0.475903\n",
            "[4000]\ttraining's l2: 0.388021\tvalid_1's l2: 0.474891\n",
            "[5000]\ttraining's l2: 0.372619\tvalid_1's l2: 0.474262\n",
            "[6000]\ttraining's l2: 0.358826\tvalid_1's l2: 0.47449\n",
            "Early stopping, best iteration is:\n",
            "[5002]\ttraining's l2: 0.372597\tvalid_1's l2: 0.47425\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[1000]\ttraining's l2: 0.47361\tvalid_1's l2: 0.4839\n",
            "[2000]\ttraining's l2: 0.433064\tvalid_1's l2: 0.462219\n",
            "[3000]\ttraining's l2: 0.410658\tvalid_1's l2: 0.457989\n",
            "[4000]\ttraining's l2: 0.392859\tvalid_1's l2: 0.456091\n",
            "[5000]\ttraining's l2: 0.377706\tvalid_1's l2: 0.455416\n",
            "[6000]\ttraining's l2: 0.364058\tvalid_1's l2: 0.455285\n",
            "Early stopping, best iteration is:\n",
            "[5815]\ttraining's l2: 0.3665\tvalid_1's l2: 0.455119\n",
            "fold n°4\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[1000]\ttraining's l2: 0.471715\tvalid_1's l2: 0.496877\n",
            "[2000]\ttraining's l2: 0.431956\tvalid_1's l2: 0.472828\n",
            "[3000]\ttraining's l2: 0.409505\tvalid_1's l2: 0.467016\n",
            "[4000]\ttraining's l2: 0.391659\tvalid_1's l2: 0.464929\n",
            "[5000]\ttraining's l2: 0.376239\tvalid_1's l2: 0.464048\n",
            "[6000]\ttraining's l2: 0.36213\tvalid_1's l2: 0.463628\n",
            "[7000]\ttraining's l2: 0.349338\tvalid_1's l2: 0.463767\n",
            "Early stopping, best iteration is:\n",
            "[6272]\ttraining's l2: 0.358584\tvalid_1's l2: 0.463542\n",
            "fold n°5\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "[1000]\ttraining's l2: 0.466349\tvalid_1's l2: 0.507696\n",
            "[2000]\ttraining's l2: 0.425606\tvalid_1's l2: 0.492745\n",
            "[3000]\ttraining's l2: 0.403731\tvalid_1's l2: 0.488917\n",
            "[4000]\ttraining's l2: 0.386479\tvalid_1's l2: 0.487113\n",
            "[5000]\ttraining's l2: 0.371358\tvalid_1's l2: 0.485881\n",
            "[6000]\ttraining's l2: 0.357821\tvalid_1's l2: 0.485185\n",
            "[7000]\ttraining's l2: 0.345577\tvalid_1's l2: 0.484535\n",
            "[8000]\ttraining's l2: 0.33415\tvalid_1's l2: 0.484483\n",
            "Early stopping, best iteration is:\n",
            "[7649]\ttraining's l2: 0.338078\tvalid_1's l2: 0.484416\n",
            "CV score: 0.47052692\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. xgboost"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "##### xgb_49\n",
        "xgb_49_params = {'eta': 0.02, \n",
        "              'max_depth': 5, \n",
        "              'min_child_weight':3,\n",
        "              'gamma':0,\n",
        "              'subsample': 0.7, \n",
        "              'colsample_bytree': 0.35, \n",
        "              'lambda':2,\n",
        "              'objective': 'reg:linear', \n",
        "              'eval_metric': 'rmse', \n",
        "              'silent': True, \n",
        "              'nthread': -1}\n",
        "\n",
        "\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
        "oof_xgb_49 = np.zeros(len(X_train_49))\n",
        "predictions_xgb_49 = np.zeros(len(X_test_49))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    trn_data = xgb.DMatrix(X_train_49[trn_idx], y_train[trn_idx])\n",
        "    val_data = xgb.DMatrix(X_train_49[val_idx], y_train[val_idx])\n",
        "\n",
        "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
        "    xgb_49 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_49_params)\n",
        "    oof_xgb_49[val_idx] = xgb_49.predict(xgb.DMatrix(X_train_49[val_idx]), ntree_limit=xgb_49.best_ntree_limit)\n",
        "    predictions_xgb_49 += xgb_49.predict(xgb.DMatrix(X_test_49), ntree_limit=xgb_49.best_ntree_limit) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_49, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "[19:25:31] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:25:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
            "Parameters: { silent } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "[0]\ttrain-rmse:3.40431\tvalid_data-rmse:3.38307\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.52770\tvalid_data-rmse:0.72110\n",
            "[1000]\ttrain-rmse:0.43563\tvalid_data-rmse:0.72245\n",
            "Stopping. Best iteration:\n",
            "[690]\ttrain-rmse:0.49010\tvalid_data-rmse:0.72044\n",
            "\n",
            "[19:25:44] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "fold n°2\n",
            "[19:25:44] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:25:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
            "Parameters: { silent } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "[0]\ttrain-rmse:3.39815\tvalid_data-rmse:3.40784\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.52871\tvalid_data-rmse:0.70336\n",
            "[1000]\ttrain-rmse:0.43793\tvalid_data-rmse:0.70446\n",
            "Stopping. Best iteration:\n",
            "[754]\ttrain-rmse:0.47982\tvalid_data-rmse:0.70278\n",
            "\n",
            "[19:25:57] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "fold n°3\n",
            "[19:25:57] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:25:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
            "Parameters: { silent } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "[0]\ttrain-rmse:3.40183\tvalid_data-rmse:3.39291\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.53169\tvalid_data-rmse:0.66896\n",
            "[1000]\ttrain-rmse:0.44129\tvalid_data-rmse:0.67058\n",
            "Stopping. Best iteration:\n",
            "[452]\ttrain-rmse:0.54177\tvalid_data-rmse:0.66871\n",
            "\n",
            "[19:26:07] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "fold n°4\n",
            "[19:26:07] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:26:07] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
            "Parameters: { silent } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "[0]\ttrain-rmse:3.40240\tvalid_data-rmse:3.39014\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.53218\tvalid_data-rmse:0.67783\n",
            "[1000]\ttrain-rmse:0.44361\tvalid_data-rmse:0.67978\n",
            "Stopping. Best iteration:\n",
            "[566]\ttrain-rmse:0.51924\tvalid_data-rmse:0.67765\n",
            "\n",
            "[19:26:18] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "fold n°5\n",
            "[19:26:19] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:26:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
            "Parameters: { silent } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "[0]\ttrain-rmse:3.39345\tvalid_data-rmse:3.42619\n",
            "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
            "\n",
            "Will train until valid_data-rmse hasn't improved in 600 rounds.\n",
            "[500]\ttrain-rmse:0.53565\tvalid_data-rmse:0.66150\n",
            "[1000]\ttrain-rmse:0.44204\tvalid_data-rmse:0.66241\n",
            "Stopping. Best iteration:\n",
            "[747]\ttrain-rmse:0.48554\tvalid_data-rmse:0.66016\n",
            "\n",
            "[19:26:32] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "CV score: 0.47102840\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. GradientBoostingRegressor梯度提升决策树"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
        "oof_gbr_49 = np.zeros(train_shape)\n",
        "predictions_gbr_49 = np.zeros(len(X_test_49))\n",
        "#GradientBoostingRegressor梯度提升决策树\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_49[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    gbr_49 = gbr(n_estimators=600, learning_rate=0.01,subsample=0.65,max_depth=6, min_samples_leaf=20,\n",
        "            max_features=0.35,verbose=1)\n",
        "    gbr_49.fit(tr_x,tr_y)\n",
        "    oof_gbr_49[val_idx] = gbr_49.predict(X_train_49[val_idx])\n",
        "    \n",
        "    predictions_gbr_49 += gbr_49.predict(X_test_49) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_49, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6529           0.0032            9.69s\n",
            "         2           0.6736           0.0029            9.55s\n",
            "         3           0.6522           0.0029            9.29s\n",
            "         4           0.6393           0.0034            9.49s\n",
            "         5           0.6454           0.0032            9.36s\n",
            "         6           0.6467           0.0031            9.22s\n",
            "         7           0.6650           0.0026            9.23s\n",
            "         8           0.6225           0.0030            9.20s\n",
            "         9           0.6350           0.0028            9.09s\n",
            "        10           0.6311           0.0028            9.25s\n",
            "        20           0.6074           0.0022            8.67s\n",
            "        30           0.5790           0.0017            8.19s\n",
            "        40           0.5443           0.0016            7.89s\n",
            "        50           0.5405           0.0013            7.63s\n",
            "        60           0.5141           0.0010            7.47s\n",
            "        70           0.4991           0.0008            7.28s\n",
            "        80           0.4791           0.0007            7.12s\n",
            "        90           0.4707           0.0006            6.92s\n",
            "       100           0.4632           0.0006            6.74s\n",
            "       200           0.4013           0.0001            5.09s\n",
            "       300           0.3924          -0.0001            3.62s\n",
            "       400           0.3526          -0.0000            2.32s\n",
            "       500           0.3355          -0.0000            1.12s\n",
            "       600           0.3201          -0.0000            0.00s\n",
            "fold n°2\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6518           0.0034            8.83s\n",
            "         2           0.6618           0.0033            8.42s\n",
            "         3           0.6483           0.0032            8.28s\n",
            "         4           0.6592           0.0029            8.27s\n",
            "         5           0.6386           0.0030            8.18s\n",
            "         6           0.6438           0.0031            8.16s\n",
            "         7           0.6477           0.0033            8.12s\n",
            "         8           0.6593           0.0029            8.15s\n",
            "         9           0.6182           0.0029            8.19s\n",
            "        10           0.6358           0.0028            8.32s\n",
            "        20           0.5810           0.0025            7.91s\n",
            "        30           0.5816           0.0020            7.74s\n",
            "        40           0.5529           0.0013            7.53s\n",
            "        50           0.5402           0.0011            7.38s\n",
            "        60           0.5096           0.0011            7.17s\n",
            "        70           0.4883           0.0010            7.03s\n",
            "        80           0.4980           0.0007            6.84s\n",
            "        90           0.4706           0.0006            6.71s\n",
            "       100           0.4704           0.0004            6.55s\n",
            "       200           0.3867           0.0001            5.01s\n",
            "       300           0.3686          -0.0000            3.60s\n",
            "       400           0.3363          -0.0000            2.32s\n",
            "       500           0.3357          -0.0000            1.13s\n",
            "       600           0.3160          -0.0000            0.00s\n",
            "fold n°3\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6457           0.0038            8.04s\n",
            "         2           0.6687           0.0033            8.08s\n",
            "         3           0.6462           0.0036            8.04s\n",
            "         4           0.6587           0.0035            8.02s\n",
            "         5           0.6430           0.0031            7.99s\n",
            "         6           0.6540           0.0029            7.95s\n",
            "         7           0.6377           0.0030            7.93s\n",
            "         8           0.6414           0.0030            7.97s\n",
            "         9           0.6399           0.0030            8.07s\n",
            "        10           0.6375           0.0028            8.07s\n",
            "        20           0.5949           0.0025            7.67s\n",
            "        30           0.5854           0.0019            7.72s\n",
            "        40           0.5386           0.0016            7.46s\n",
            "        50           0.5156           0.0013            7.32s\n",
            "        60           0.5080           0.0011            7.17s\n",
            "        70           0.5021           0.0009            7.04s\n",
            "        80           0.4654           0.0008            6.85s\n",
            "        90           0.4712           0.0006            6.72s\n",
            "       100           0.4740           0.0006            6.53s\n",
            "       200           0.3924           0.0000            4.96s\n",
            "       300           0.3568          -0.0000            3.58s\n",
            "       400           0.3400          -0.0001            2.31s\n",
            "       500           0.3283          -0.0001            1.12s\n",
            "       600           0.3044          -0.0000            0.00s\n",
            "fold n°4\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6606           0.0032            8.27s\n",
            "         2           0.6878           0.0030            8.37s\n",
            "         3           0.6490           0.0031            8.37s\n",
            "         4           0.6564           0.0032            8.29s\n",
            "         5           0.6568           0.0027            8.27s\n",
            "         6           0.6496           0.0030            8.27s\n",
            "         7           0.6451           0.0029            8.22s\n",
            "         8           0.6210           0.0031            8.21s\n",
            "         9           0.6239           0.0028            8.35s\n",
            "        10           0.6535           0.0025            8.35s\n",
            "        20           0.6038           0.0022            7.92s\n",
            "        30           0.6032           0.0019            7.76s\n",
            "        40           0.5492           0.0018            7.55s\n",
            "        50           0.5333           0.0011            7.37s\n",
            "        60           0.4973           0.0010            7.24s\n",
            "        70           0.4942           0.0009            7.09s\n",
            "        80           0.4753           0.0008            6.92s\n",
            "        90           0.4806           0.0005            6.76s\n",
            "       100           0.4659           0.0005            6.58s\n",
            "       200           0.4046           0.0000            4.99s\n",
            "       300           0.3647          -0.0000            3.59s\n",
            "       400           0.3561          -0.0000            2.32s\n",
            "       500           0.3330          -0.0000            1.12s\n",
            "       600           0.3152          -0.0000            0.00s\n",
            "fold n°5\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.6721           0.0036            8.28s\n",
            "         2           0.6822           0.0034            8.41s\n",
            "         3           0.6634           0.0033            8.26s\n",
            "         4           0.6584           0.0032            8.21s\n",
            "         5           0.6574           0.0030            8.40s\n",
            "         6           0.6544           0.0033            8.31s\n",
            "         7           0.6533           0.0028            8.30s\n",
            "         8           0.6196           0.0029            8.27s\n",
            "         9           0.6530           0.0028            8.43s\n",
            "        10           0.6108           0.0032            8.49s\n",
            "        20           0.6107           0.0027            7.91s\n",
            "        30           0.5649           0.0020            7.70s\n",
            "        40           0.5555           0.0016            7.55s\n",
            "        50           0.5156           0.0014            7.40s\n",
            "        60           0.5144           0.0010            7.21s\n",
            "        70           0.5001           0.0009            7.05s\n",
            "        80           0.4908           0.0007            6.88s\n",
            "        90           0.4820           0.0008            6.73s\n",
            "       100           0.4617           0.0007            6.55s\n",
            "       200           0.3993          -0.0000            5.01s\n",
            "       300           0.3678          -0.0000            3.61s\n",
            "       400           0.3399          -0.0000            2.31s\n",
            "       500           0.3182          -0.0000            1.12s\n",
            "       600           0.3238          -0.0000            0.00s\n",
            "CV score: 0.46724198\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此，我们得到了以上3种模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_stack3 = np.vstack([oof_lgb_49,oof_xgb_49,oof_gbr_49]).transpose()\n",
        "test_stack3 = np.vstack([predictions_lgb_49, predictions_xgb_49,predictions_gbr_49]).transpose()\n",
        "#\n",
        "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
        "oof_stack3 = np.zeros(train_stack3.shape[0])\n",
        "predictions_lr3 = np.zeros(test_stack3.shape[0])\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack3,target)):\n",
        "    print(\"fold {}\".format(fold_))\n",
        "    trn_data, trn_y = train_stack3[trn_idx], target.iloc[trn_idx].values\n",
        "    val_data, val_y = train_stack3[val_idx], target.iloc[val_idx].values\n",
        "        #Kernel Ridge Regression\n",
        "    lr3 = kr()\n",
        "    lr3.fit(trn_data, trn_y)\n",
        "    \n",
        "    oof_stack3[val_idx] = lr3.predict(val_data)\n",
        "    predictions_lr3 += lr3.predict(test_stack3) / 10\n",
        "    \n",
        "mean_squared_error(target.values, oof_stack3) \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0\n",
            "fold 1\n",
            "fold 2\n",
            "fold 3\n",
            "fold 4\n",
            "fold 5\n",
            "fold 6\n",
            "fold 7\n",
            "fold 8\n",
            "fold 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "0.4662728551415085"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下来我们对于383维的数据进行与上述263以及49维数据相同的操作\n",
        "\n",
        "1. Kernel Ridge Regression 基于核的岭回归"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_kr_383 = np.zeros(train_shape)\n",
        "predictions_kr_383 = np.zeros(len(X_test_383))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_383[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    #Kernel Ridge Regression 岭回归\n",
        "    kr_383 = kr()\n",
        "    kr_383.fit(tr_x,tr_y)\n",
        "    oof_kr_383[val_idx] = kr_383.predict(X_train_383[val_idx])\n",
        "    \n",
        "    predictions_kr_383 += kr_383.predict(X_test_383) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_383, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.51412085\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 使用普通岭回归"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_ridge_383 = np.zeros(train_shape)\n",
        "predictions_ridge_383 = np.zeros(len(X_test_383))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_383[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    #使用岭回归\n",
        "    ridge_383 = Ridge(alpha=1200)\n",
        "    ridge_383.fit(tr_x,tr_y)\n",
        "    oof_ridge_383[val_idx] = ridge_383.predict(X_train_383[val_idx])\n",
        "    \n",
        "    predictions_ridge_383 += ridge_383.predict(X_test_383) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_383, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.48687670\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 使用ElasticNet 弹性网络"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_en_383 = np.zeros(train_shape)\n",
        "predictions_en_383 = np.zeros(len(X_test_383))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_383[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    #ElasticNet 弹性网络\n",
        "    en_383 = en(alpha=1.0,l1_ratio=0.06)\n",
        "    en_383.fit(tr_x,tr_y)\n",
        "    oof_en_383[val_idx] = en_383.predict(X_train_383[val_idx])\n",
        "    \n",
        "    predictions_en_383 += en_383.predict(X_test_383) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_383, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.53296555\n"
          ]
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 使用BayesianRidge 贝叶斯岭回归"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_br_383 = np.zeros(train_shape)\n",
        "predictions_br_383 = np.zeros(len(X_test_383))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_383[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    #BayesianRidge 贝叶斯回归\n",
        "    br_383 = br()\n",
        "    br_383.fit(tr_x,tr_y)\n",
        "    oof_br_383[val_idx] = br_383.predict(X_train_383[val_idx])\n",
        "    \n",
        "    predictions_br_383 += br_383.predict(X_test_383) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_383, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.48717310\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此，我们得到了以上4种模型的基于383个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_stack1 = np.vstack([oof_br_383,oof_kr_383,oof_en_383,oof_ridge_383]).transpose()\n",
        "test_stack1 = np.vstack([predictions_br_383, predictions_kr_383,predictions_en_383,predictions_ridge_383]).transpose()\n",
        "\n",
        "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
        "oof_stack1 = np.zeros(train_stack1.shape[0])\n",
        "predictions_lr1 = np.zeros(test_stack1.shape[0])\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack1,target)):\n",
        "    print(\"fold {}\".format(fold_))\n",
        "    trn_data, trn_y = train_stack1[trn_idx], target.iloc[trn_idx].values\n",
        "    val_data, val_y = train_stack1[val_idx], target.iloc[val_idx].values\n",
        "    # LinearRegression简单的线性回归\n",
        "    lr1 = lr()\n",
        "    lr1.fit(trn_data, trn_y)\n",
        "    \n",
        "    oof_stack1[val_idx] = lr1.predict(val_data)\n",
        "    predictions_lr1 += lr1.predict(test_stack1) / 10\n",
        "    \n",
        "mean_squared_error(target.values, oof_stack1) \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0\n",
            "fold 1\n",
            "fold 2\n",
            "fold 3\n",
            "fold 4\n",
            "fold 5\n",
            "fold 6\n",
            "fold 7\n",
            "fold 8\n",
            "fold 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "0.4878202780283125"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于49维的特征是最重要的特征，所以这里考虑增加更多的模型进行49维特征的数据的构建工作。\n",
        "1. KernelRidge 核岭回归"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_kr_49 = np.zeros(train_shape)\n",
        "predictions_kr_49 = np.zeros(len(X_test_49))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_49[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    kr_49 = kr()\n",
        "    kr_49.fit(tr_x,tr_y)\n",
        "    oof_kr_49[val_idx] = kr_49.predict(X_train_49[val_idx])\n",
        "    \n",
        "    predictions_kr_49 += kr_49.predict(X_test_49) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_49, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.50254410\n"
          ]
        }
      ],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ridge 岭回归"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_ridge_49 = np.zeros(train_shape)\n",
        "predictions_ridge_49 = np.zeros(len(X_test_49))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_49[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    ridge_49 = Ridge(alpha=6)\n",
        "    ridge_49.fit(tr_x,tr_y)\n",
        "    oof_ridge_49[val_idx] = ridge_49.predict(X_train_49[val_idx])\n",
        "    \n",
        "    predictions_ridge_49 += ridge_49.predict(X_test_49) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_49, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.49451286\n"
          ]
        }
      ],
      "execution_count": 32,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. BayesianRidge 贝叶斯岭回归"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_br_49 = np.zeros(train_shape)\n",
        "predictions_br_49 = np.zeros(len(X_test_49))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_49[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    br_49 = br()\n",
        "    br_49.fit(tr_x,tr_y)\n",
        "    oof_br_49[val_idx] = br_49.predict(X_train_49[val_idx])\n",
        "    \n",
        "    predictions_br_49 += br_49.predict(X_test_49) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_49, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.49534595\n"
          ]
        }
      ],
      "execution_count": 33,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ElasticNet 弹性网络"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
        "oof_en_49 = np.zeros(train_shape)\n",
        "predictions_en_49 = np.zeros(len(X_test_49))\n",
        "#\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
        "    print(\"fold n°{}\".format(fold_+1))\n",
        "    tr_x = X_train_49[trn_idx]\n",
        "    tr_y = y_train[trn_idx]\n",
        "    en_49 = en(alpha=1.0,l1_ratio=0.05)\n",
        "    en_49.fit(tr_x,tr_y)\n",
        "    oof_en_49[val_idx] = en_49.predict(X_train_49[val_idx])\n",
        "    \n",
        "    predictions_en_49 += en_49.predict(X_test_49) / folds.n_splits\n",
        "\n",
        "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_49, target)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold n°1\n",
            "fold n°2\n",
            "fold n°3\n",
            "fold n°4\n",
            "fold n°5\n",
            "CV score: 0.53841695\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们得到了以上4种新模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_stack4 = np.vstack([oof_br_49,oof_kr_49,oof_en_49,oof_ridge_49]).transpose()\n",
        "test_stack4 = np.vstack([predictions_br_49, predictions_kr_49,predictions_en_49,predictions_ridge_49]).transpose()\n",
        "\n",
        "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
        "oof_stack4 = np.zeros(train_stack4.shape[0])\n",
        "predictions_lr4 = np.zeros(test_stack4.shape[0])\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack4,target)):\n",
        "    print(\"fold {}\".format(fold_))\n",
        "    trn_data, trn_y = train_stack4[trn_idx], target.iloc[trn_idx].values\n",
        "    val_data, val_y = train_stack4[val_idx], target.iloc[val_idx].values\n",
        "    #LinearRegression\n",
        "    lr4 = lr()\n",
        "    lr4.fit(trn_data, trn_y)\n",
        "    \n",
        "    oof_stack4[val_idx] = lr4.predict(val_data)\n",
        "    predictions_lr4 += lr4.predict(test_stack1) / 10\n",
        "    \n",
        "mean_squared_error(target.values, oof_stack4) \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0\n",
            "fold 1\n",
            "fold 2\n",
            "fold 3\n",
            "fold 4\n",
            "fold 5\n",
            "fold 6\n",
            "fold 7\n",
            "fold 8\n",
            "fold 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "0.49491439094008133"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型融合\n",
        "\n",
        "这里对于上述四种集成学习的模型的预测结果进行加权的求和，得到最终的结果，当然这种方式是很不准确的。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#和下面作对比\n",
        "mean_squared_error(target.values, 0.7*(0.6*oof_stack2 + 0.4*oof_stack3)+0.3*(0.55*oof_stack1+0.45*oof_stack4))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "0.4527515432292745"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "更好的方式是将以上的4中集成学习模型再次进行集成学习的训练，这里直接使用LinearRegression简单线性回归的进行集成。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_stack5 = np.vstack([oof_stack1,oof_stack2,oof_stack3,oof_stack4]).transpose()\n",
        "test_stack5 = np.vstack([predictions_lr1, predictions_lr2,predictions_lr3,predictions_lr4]).transpose()\n",
        "\n",
        "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
        "oof_stack5 = np.zeros(train_stack5.shape[0])\n",
        "predictions_lr5= np.zeros(test_stack5.shape[0])\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack5,target)):\n",
        "    print(\"fold {}\".format(fold_))\n",
        "    trn_data, trn_y = train_stack5[trn_idx], target.iloc[trn_idx].values\n",
        "    val_data, val_y = train_stack5[val_idx], target.iloc[val_idx].values\n",
        "    #LinearRegression\n",
        "    lr5 = lr()\n",
        "    lr5.fit(trn_data, trn_y)\n",
        "    \n",
        "    oof_stack5[val_idx] = lr5.predict(val_data)\n",
        "    predictions_lr5 += lr5.predict(test_stack5) / 10\n",
        "    \n",
        "mean_squared_error(target.values, oof_stack5) \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0\n",
            "fold 1\n",
            "fold 2\n",
            "fold 3\n",
            "fold 4\n",
            "fold 5\n",
            "fold 6\n",
            "fold 7\n",
            "fold 8\n",
            "fold 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "0.4480223491250565"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 结果保存\n",
        "\n",
        "进行index的读取工作"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "submit_example = pd.read_csv('submit_example.csv',sep=',',encoding='latin-1')\n",
        "\n",
        "submit_example['happiness'] = predictions_lr5\n",
        "\n",
        "submit_example.happiness.describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "count    2968.000000\nmean        3.879322\nstd         0.462290\nmin         1.636433\n25%         3.667859\n50%         3.954825\n75%         4.185277\nmax         5.051027\nName: happiness, dtype: float64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "进行结果保存，这里我们预测出的值是1-5的连续值，但是我们的ground truth是整数值，所以为了进一步优化我们的结果，我们对于结果进行了整数解的近似，并保存到了csv文件中。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "submit_example.loc[submit_example['happiness']>4.96,'happiness']= 5\n",
        "submit_example.loc[submit_example['happiness']<=1.04,'happiness']= 1\n",
        "submit_example.loc[(submit_example['happiness']>1.96)&(submit_example['happiness']<2.04),'happiness']= 2\n",
        "\n",
        "submit_example.to_csv(\"submision.csv\",index=False)\n",
        "submit_example.happiness.describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "count    2968.000000\nmean        3.879330\nstd         0.462127\nmin         1.636433\n25%         3.667859\n50%         3.954825\n75%         4.185277\nmax         5.000000\nName: happiness, dtype: float64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 39,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "大家可以对于model的参数进行更进一步的调整，例如使用网格搜索的方法。这留给大家做进一步的思考喽～"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "iris = load_iris()\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=0)\n",
        "print(\"Size of training set:{} size of testing set:{}\".format(X_train.shape[0],X_test.shape[0]))\n",
        " \n",
        "####   1\n",
        "best_score = 0\n",
        "for gamma in [0.001,0.01,0.1,1,10,100]:\n",
        "    for C in [0.001,0.01,0.1,1,10,100]:\n",
        "        svm = SVC(gamma=gamma,C=C)#对于每种参数可能的组合，进行一次训练；\n",
        "        svm.fit(X_train,y_train)\n",
        "        score = svm.score(X_test,y_test)\n",
        "        if score > best_score:#找到表现最好的参数\n",
        "            best_score = score\n",
        "            best_parameters = {'gamma':gamma,'C':C}\n",
        "print(\"Best score:{:.2f}\".format(best_score))\n",
        "\n",
        " \n",
        "\n",
        "####   2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "#把要调整的参数以及其候选值 列出来；\n",
        "param_grid = {\"gamma\":[0.001,0.01,0.1,1,10,100],\n",
        "             \"C\":[0.001,0.01,0.1,1,10,100]}\n",
        "print(\"Parameters:{}\".format(param_grid))\n",
        " \n",
        "grid_search = GridSearchCV(SVC(),param_grid,cv=5) #实例化一个GridSearchCV类,cv交叉验证参数\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=10)\n",
        "grid_search.fit(X_train,y_train) #训练，找到最优的参数，同时使用最优的参数实例化一个新的SVC estimator。\n",
        "print(\"Test set score:{:.2f}\".format(grid_search.score(X_test,y_test)))\n",
        "print(\"Best parameters:{}\".format(grid_search.best_params_))\n",
        "\n",
        "#SVM模型有两个非常重要的参数C与gamma。\n",
        "#C是惩罚系数，即对误差的容忍度（间隔大小,分类准确度）。C越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差\n",
        "#gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值越小，支持向量越多。支持向量的个数影响训练与预测的速度。\n",
        "#两者独立\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "Result:\n",
        "    \n",
        "Parameters:{'gamma': [0.001, 0.01, 0.1, 1, 10, 100], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "Test set score:0.97\n",
        "Best parameters:{'C': 10, 'gamma': 0.1}\n",
        "Best score on train set:0.98\n",
        "\n",
        "#Grid Search 调参方法存在的共性弊端就是：耗时；参数越多，候选值越多，耗费时间越长！所以，一般情况下，先定一个大范围，然后再细化。"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}