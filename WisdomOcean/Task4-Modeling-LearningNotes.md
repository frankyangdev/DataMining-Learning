
### Notebook ###

运行结果 [T4-BuildModeling.ipynb](https://github.com/frankyangdev/DataMining-Learning/blob/main/WisdomOcean/T4-BuildModeling.ipynb)

### [XGBoost模型](https://blog.csdn.net/wuzhongqiang/article/details/104854890) ###

常见的机器学习算法：

* 监督学习算法：逻辑回归，线性回归，决策树，朴素贝叶斯，K近邻，支持向量机，集成算法Adaboost等
* 无监督算法：聚类，降维，关联规则, PageRank等

根据各个弱分类器之间有无依赖关系，分为Boosting和Bagging

* Boosting流派，各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT(Gradient Boosting Decision Tree)、Xgboost
* Bagging流派，各分类器之间没有依赖关系，可各自并行，比如随机森林（Random Forest）

AdaBoost，是英文"Adaptive Boosting"（自适应增强），它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。白话的讲，就是它在训练弱分类器之前，会给每个样本一个权重，训练完了一个分类器，就会调整样本的权重，前一个分类器分错的样本权重会加大，这样后面再训练分类器的时候，就会更加注重前面分错的样本， 然后一步一步的训练出很多个弱分类器，最后，根据弱分类器的表现给它们加上权重，组合成一个强大的分类器，就足可以应付整个数据集了。 这就是AdaBoost， 它强调自适应，不断修改样本权重， 不断加入弱分类器进行boosting。

GBDT(Gradient Boost Decision Tree)就是另一种boosting的方式， 上面说到AdaBoost训练弱分类器关注的是那些被分错的样本，AdaBoost每一次训练都是为了减少错误分类的样本。 而GBDT训练弱分类器关注的是残差，也就是上一个弱分类器的表现与完美答案之间的差距，GBDT每一次训练分类器，都是为了减少这个差距

xgboost与gbdt比较大的不同就是目标函数的定义，但这俩在策略上是类似的，都是聚焦残差（更准确的说， xgboost其实是gbdt算法在工程上的一种实现方式），GBDT旨在通过不断加入新的树最快速度降低残差，而XGBoost则可以人为定义损失函数（可以是最小平方差、logistic loss function、hinge loss function或者人为定义的loss function），只需要知道该loss function对参数的一阶、二阶导数便可以进行boosting，其进一步增大了模型的泛化能力，其贪婪法寻找添加树的结构以及loss function中的损失函数与正则项等一系列策略也使得XGBoost预测更准确。

![image](https://user-images.githubusercontent.com/39177230/112449276-2de84000-8d8e-11eb-967e-0f2a7bc38309.png)

xgboost相比于GBDT有哪些优点：

* 精度更高：GBDT只用到一阶泰勒， 而xgboost对损失函数进行了二阶泰勒展开， 一方面为了增加精度， 另一方面也为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数
* 灵活性更强：GBDT以CART作为基分类器，而Xgboost不仅支持CART，还支持线性分类器，另外，Xgboost支持自定义损失函数，只要损失函数有一二阶导数。
* 正则化：xgboost在目标函数中加入了正则，用于控制模型的复杂度。有助于降低模型方差，防止过拟合。正则项里包含了树的叶子节点个数，叶子节点权重的L2范式。
* Shrinkage（缩减）：相当于学习速率。这个主要是为了削弱每棵树的影响，让后面有更大的学习空间，学习过程更加的平缓
* 列抽样：这个就是在建树的时候，不用遍历所有的特征了，可以进行抽样，一方面简化了计算，另一方面也有助于降低过拟合
* 缺失值处理：这个是xgboost的稀疏感知算法，加快了节点分裂的速度
* 并行化操作：块结构可以很好的支持并行计算


### [LightGBM模型](https://blog.csdn.net/wuzhongqiang/article/details/105350579)

xgboost寻找最优分裂点的复杂度=特征数量×分裂点的数量×样本的数量

Lightgbm里面的直方图算法就是为了减少分裂点的数量， Lightgbm里面的单边梯度抽样算法就是为了减少样本的数量， 而Lightgbm里面的互斥特征捆绑算法就是为了减少特征的数量。 并且后面两个是Lightgbm的亮点所在。

与xgboost对比一下，总结一下lightgbm的优点作为收尾， 从内存和速度两方面总结：

* 内存更小
* XGBoost 使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 O(2*#data) 降低为 O(#bin) ，极大的减少了内存消耗；
* LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗；
* LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。
* 速度更快
* LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；
* LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；
* LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；
* LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；
* LightGBM 对缓存也进行了优化，增加了 Cache hit 的命中率。


### 交叉验证(Cross validation) ###

在模型建立中，通常有两个数据集：训练集（train）和测试集（test）。训练集用来训练模型；测试集是完全不参与训练的数据，仅仅用来观测测试效果的数据。
一般情况下，训练的结果对于训练集的拟合程度通常还是挺好的，但是在测试集总的表现却可能不行。比如下面的例子：

![image](https://user-images.githubusercontent.com/39177230/115723986-1a42f000-a3b3-11eb-85f5-49852c0ac824.png)

* 图一的模型是一条线型方程。 可以看到，所有的红点都不在蓝线上，所以导致了错误率很高，这是典型的不拟合的情况
* 图二 的蓝线则更加贴近实际的红点，虽然没有完全重合，但是可以看出模型表示的关系是正确的。
* 图三，所有点都在蓝线上，这时候模型计算出的错误率很低，（甚至将噪音都考虑进去了）。这个模型只在训练集中表现很好，在测试集中的表现就不行。 这是典型的‘过拟合’情况。

**交叉验证:** 就是在训练集中选一部分样本用于测试模型。保留一部分的训练集数据作为验证集/评估集，对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。

### 留一验证（LOOCV，Leave one out cross validation ）###

只从可用的数据集中保留一个数据点，并根据其余数据训练模型。此过程对每个数据点进行迭代，比如有n个数据点，就要重复交叉验证n次。例如下图，一共10个数据，就交叉验证十次

![image](https://user-images.githubusercontent.com/39177230/115724595-ac4af880-a3b3-11eb-95d3-2aabc1cc0781.png)

**优点:**

* 适合小样本数据集
* 利用所有的数据点，因此偏差将很低

**缺点:**

* 重复交叉验证过程n次导致更高的执行时间
* 测试模型有效性的变化大。因为针对一个数据点进行测试，模型的估计值受到数据点的很大影响。如果数据点被证明是一个离群值，它可能导致更大的变化

LOOCC是保留一个数据点，同样的你也可以保留P个数据点作为验证集，这种方法叫LPOCV(Leave P Out Cross Validation)

### 验证集方法 ###

* 保留一个样本数据集， （取出训练集中20%的样本不用）
* 使用数据集的剩余部分训练模型 （使用另外的80%样本训练模型）
* 使用验证集的保留样本。（完成模型后，在20%的样本中测试）
* 如果模型在验证数据上提供了一个肯定的结果，那么继续使用当前的模型。

**优点：**

简单方便。直接将训练集按比例拆分成训练集和验证集，比如50:50。

**缺点：**

没有充分利用数据， 结果具有偶然性。如果按50:50分，会损失掉另外50%的数据信息，因为我们没有利用着50%的数据来训练模型。

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

#已经导入数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, shuffle=True)

# Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)
prediction = model.predict(X_test)
print('The accuracy of the Logistic Regression is: {0}'.format(metrics.accuracy_score(prediction,y_test)))

```

### K折交叉验证（k-fold cross validation） ###

针对上面通过train_test_split划分，从而进行模型评估方式存在的弊端，提出Cross Validation 交叉验证。

Cross Validation：简言之，就是进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练、测试评估，从而得出一个评价结果；如果是5折交叉验证，意思就是在原始数据集上，进行5次划分，每次划分进行一次训练、评估，最后得到5次划分后的评估结果，一般在这几次评估结果上取平均得到最后的 评分。k-fold cross-validation ，其中，k一般取5或10。


![image](https://user-images.githubusercontent.com/39177230/115725970-e072e900-a3b4-11eb-9171-600fa42a5f0d.png)


训练模型需要在大量的数据集基础上，否则就不能够识别数据中的趋势，导致错误产生
同样需要适量的验证数据点。 验证集太小容易导致误差
多次训练和验证模型。需要改变训练集和验证集的划分，有助于验证模型。
步骤：
随机将整个数据集分成k折；
如图中所示，依次取每一折的数据集作验证集，剩余部分作为训练集
算出每一折测试的错误率
取这里K次的记录平均值 作为最终结果

**优点:**

* 适合大样本的数据集
* 经过多次划分，大大降低了结果的偶然性，从而提高了模型的准确性。
* 对数据的使用效率更高。train_test_split，默认训练集、测试集比例为3:1。如果是5折交叉验证，训练集比测试集为4:1；10折交叉验证训练集比测试集为9:1。数据量越大，模型准确率越高。

**缺点：**

* 对数据随机均等划分，不适合包含不同类别的数据集。比如：数据集有5类数据（ABCDE各占20%），抽取出来的也正好是按照类别划分的5类，第一折全是A，第二折全是B……这样就会导致，模型学习到测试集中数据的特点，用BCDE训练的模型去测试A类数据、ACDE的模型测试B类数据，这样准确率就会很低。

***如何确定K值？**

* 一般情况下3、5是默认选项，常建议用K=10。
* k值越低，就越有偏差；K值越高偏差就越小，但是会受到很大的变化。
* k值越小，就越类似于验证集方法；而k值越大，则越接近LOOCV方法。

### 分层交叉验证 （Stratified k-fold cross validation）###

分层是重新将数据排列组合，使得每一折都能比较好地代表整体。

标准交叉验证和分层交叉验证的区别：

* 标准交叉验证（即K折交叉验证）：直接将数据分成几折；
* 分层交叉验证：先将数据分类（class1,2,3)，然后在每个类别中划分三折。

![image](https://user-images.githubusercontent.com/39177230/115726568-6abb4d00-a3b5-11eb-9ba2-5198ba64a97d.png)


### 时间序列的交叉验证(Cross Validation for time series）###


为了解决时间序列的预测问题，可以尝试时间序列交叉验证：采用正向链接的策略，即按照时间顺序划分每一折的数据集。

从一个最小的训练集开始（这个训练集具有拟合模型所需的最少观测数）逐步地，每次都会更换训练集和测试集。在大多数情况下，不必一个个点向前移动，可以设置一次跨5个点/10个点。在回归问题中，可以使用以下代码执行交叉验证。

```python
from sklearn.model_selection import TimeSeriesSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
tscv = TimeSeriesSplit(n_splits=3)

for train_index, test_index in tscv.split(X):
     print("Train:", train_index, "Validation:", val_index)
     X_train, X_test = X[train_index], X[val_index]
     y_train, y_test = y[train_index], y[val_index]

TRAIN: [0] TEST: [1]
TRAIN: [0 1] TEST: [2]
TRAIN: [0 1 2] TEST: [3]
```

### 模型调参的三种常用方法：###

1. 贪心算法

（1）概念：

所谓贪心算法是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解。
贪心算法没有固定的算法框架，算法设计的关键是贪心策略的选择。必须注意的是，贪心算法不是对所有问题都能得到整体最优解，选择的贪心策略必须具备无后效性（即某个状态以后的过程不会影响以前的状态，只与当前状态有关。）所以，对所采用的贪心策略一定要仔细分析其是否满足无后效性。

（2）思路：

* 建立数学模型来描述问题
* 把求解的问题分成若干个子问题
* 对每个子问题求解，得到子问题的局部最优解
* 把子问题的解局部最优解合成原来问题的一个解
  
 (3)存在的问题：

* 不能保证求得的最后解是最佳的
* 不能用来求最大值或最小值的问题
* 只能求满足某些约束条件的可行解的范围

2. 网格调参

（1）概念：

一种调参的方法，当你算法模型效果不是很好时，可以通过该方法来调整参数，通过循环遍历，尝试每一种参数组合，返回最好的得分值的参数组合。每个参数都能组合在一起，循环过程就像是在网格中遍历，所以叫网格搜索。

（2）存在的问题：

原来的数据集分割为训练集和测试集之后，其中测试集起到的作用有两个，一个是用来调整参数，一个是用来评价模型的好坏，这样会导致评分值会比实际效果要好。（因为我们将测试集送到了模型里面去测试模型的好坏，而我们目的是要将训练模型应用在没使用过的数据上。

（3）解决方式：

把数据集划分三份，一份是训练集（训练数据），一份是验证集（调整参数），一份是测试集（测试模型）。

为了防止模型过拟合，我们使用交叉验证的方法。

3. 贝叶斯调参

（1）概念

贝叶斯优化通过基于目标函数的过去评估结果建立替代函数（概率模型），来找到最小化目标函数的值。贝叶斯方法与随机或网格搜索的不同之处在于，它在尝试下一组超参数时，会参考之前的评估结果，因此可以省去很多无用功。

超参数的评估代价很大，因为它要求使用待评估的超参数训练一遍模型，而许多深度学习模型动则几个小时几天才能完成训练，并评估模型，因此耗费巨大。贝叶斯调参发使用不断更新的概率模型，通过推断过去的结果来“集中”有希望的超参数。

（2）贝叶斯优化问题有四个部分：

目标函数：我们想要最小化的内容，在这里，目标函数是机器学习模型使用该组超参数在验证集上的损失。

域空间：要搜索的超参数的取值范围

优化算法：构造替代函数并选择下一个超参数值进行评估的方法。

结果历史记录：来自目标函数评估的存储结果，包括超参数和验证集上的损失。

* 贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息
* 贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸
* 贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部优最







### Reference ###

1. [交叉验证方法汇总](https://blog.csdn.net/WHYbeHERE/article/details/108192957)
2. [模型调参的三种常用方法](https://blog.csdn.net/zhangxiaolinxin/article/details/105256588)
